{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36f53b0a-18b8-4134-919a-f68a5cb55007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/27 16:54:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versi칩n de Spark: 3.5.1\n",
      "Versi칩n de MLflow: 3.5.1\n",
      "Ejecutando un trabajo simple en Spark...\n",
      "+----------+-----+\n",
      "|componente|valor|\n",
      "+----------+-----+\n",
      "|    Python|   10|\n",
      "|     Spark|   20|\n",
      "|    MLflow|   30|\n",
      "+----------+-----+\n",
      "\n",
      "Experimento y run de MLflow registrados con 칠xito.\n",
      "游끢 View run Run de Prueba at: http://localhost:5000/#/experiments/1/runs/a6ace5df062e4253be83300d072fbff9\n",
      "游빍 View experiment at: http://localhost:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Spark se conecta localmente\n",
    "# Usamos 'local[*]' para usar todos los cores disponibles en el contenedor\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Test Integrado\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Versi칩n de Spark: {spark.version}\")\n",
    "print(f\"Versi칩n de MLflow: {mlflow.__version__}\")\n",
    "\n",
    "# 2. MLflow se conecta a su servidor en localhost (dentro del contenedor)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# 3. Ejecutar un experimento de MLflow\n",
    "mlflow.set_experiment(\"Experimento_Todo_En_Uno\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"Run de Prueba\"):\n",
    "    mlflow.log_param(\"entorno\", \"Contenedor 칔nico\")\n",
    "    \n",
    "    print(\"Ejecutando un trabajo simple en Spark...\")\n",
    "    data = [(\"Python\", 10), (\"Spark\", 20), (\"MLflow\", 30)]\n",
    "    df = spark.createDataFrame(data, [\"componente\", \"valor\"])\n",
    "    \n",
    "    df.show()\n",
    "    \n",
    "    avg_value = df.toPandas()['valor'].mean()\n",
    "    mlflow.log_metric(\"valor_promedio\", avg_value)\n",
    "    \n",
    "    print(\"Experimento y run de MLflow registrados con 칠xito.\")\n",
    "\n",
    "spark.stop()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4867bb-3e40-4c91-abb4-d81a8cd4e5fb",
   "metadata": {},
   "source": [
    "## Configuraci칩n del Entorno de Spark\n",
    "El primer paso consiste en inicializar una SparkSession, que es el punto de entrada para programar con la API de Spark. El siguiente script tambi칠n automatiza la descarga del conjunto de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dd6e72b-4051-472b-b2af-de2563db90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conjunto de datos ya se encuentra en el directorio local.\n",
      "SparkSession iniciada. Versi칩n: 3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/27 16:38:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e7051f1bcc7b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TutorialComputacionDistribuida</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a693d726690>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Inicializaci칩n de la SparkSession\n",
    "# El master 'local[*]' instruye a Spark para utilizar todos los n칰cleos de CPU\n",
    "# disponibles en la m치quina local, simulando un entorno de trabajo paralelo.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TutorialComputacionDistribuida\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2. Descarga y gesti칩n del conjunto de datos\n",
    "data_dir = \"data\"\n",
    "filename = \"yellow_tripdata_2024-01.parquet\"\n",
    "filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Iniciando descarga del conjunto de datos: {filename}\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{filename}\"\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    print(\"Descarga finalizada.\")\n",
    "else:\n",
    "    print(\"El conjunto de datos ya se encuentra en el directorio local.\")\n",
    "\n",
    "# 3. Verificaci칩n del contexto de Spark\n",
    "print(f\"SparkSession iniciada. Versi칩n: {spark.version}\")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d78d83-9675-48c0-9644-6ee34a2724ac",
   "metadata": {},
   "source": [
    "## Ejercicio 1: El Cuello de Botella del Mononodo con Pandas\n",
    "Este ejercicio demuestra las limitaciones inherentes al procesamiento en un solo nodo. Se utilizar치 la librer칤a Pandas para cargar y procesar el dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75520ac9-c678-44d9-8731-ccfff24fb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el conjunto de datos en un DataFrame de Pandas...\n",
      "CPU times: user 688 ms, sys: 681 ms, total: 1.37 s\n",
      "Wall time: 113 ms\n",
      "CPU times: user 10.5 ms, sys: 0 ns, total: 10.5 ms\n",
      "Wall time: 10.5 ms\n",
      "\n",
      "--- An치lisis de Rendimiento con Pandas ---\n",
      "Consumo de memoria RAM: 0.56 GB\n",
      "\n",
      "Resultado del conteo de pasajeros:\n",
      "passenger_count\n",
      "1.0    2188739\n",
      "2.0     405103\n",
      "3.0      91262\n",
      "4.0      51974\n",
      "5.0      33506\n",
      "0.0      31465\n",
      "6.0      22353\n",
      "8.0         51\n",
      "7.0          8\n",
      "9.0          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Cargando el conjunto de datos en un DataFrame de Pandas...\")\n",
    "\n",
    "# Medici칩n del tiempo de carga y consumo de memoria\n",
    "%time df_pandas = pd.read_parquet(filepath)\n",
    "\n",
    "# Medici칩n del tiempo de una operaci칩n de agregaci칩n simple\n",
    "%time conteo_pandas = df_pandas['passenger_count'].value_counts()\n",
    "\n",
    "print(\"\\n--- An치lisis de Rendimiento con Pandas ---\")\n",
    "print(f\"Consumo de memoria RAM: {df_pandas.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
    "print(\"\\nResultado del conteo de pasajeros:\")\n",
    "print(conteo_pandas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e9d9f-e79e-4050-8588-47bb3abc1a7f",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Ingesta de Datos Distribuida y Evaluaci칩n Perezosa\n",
    "A continuaci칩n, se realiza la misma operaci칩n de carga utilizando Spark para ilustrar un concepto fundamental: la evaluaci칩n perezosa (lazy evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb48a18-2292-44ac-b4fc-db8e36948f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el conjunto de datos en un DataFrame de Spark...\n",
      "CPU times: user 1.16 ms, sys: 269 풮s, total: 1.43 ms\n",
      "Wall time: 177 ms\n",
      "CPU times: user 644 풮s, sys: 0 ns, total: 644 풮s\n",
      "Wall time: 276 ms\n",
      "\n",
      "El DataFrame de Spark contiene 2964624 registros.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando el conjunto de datos en un DataFrame de Spark...\")\n",
    "\n",
    "# La lectura del archivo es una transformaci칩n perezosa. Es instant치nea.\n",
    "%time df_spark = spark.read.parquet(filepath)\n",
    "\n",
    "# .count() es una acci칩n, la cual dispara la ejecuci칩n del trabajo.\n",
    "%time total_filas = df_spark.count()\n",
    "\n",
    "print(f\"\\nEl DataFrame de Spark contiene {total_filas} registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef89e8-1051-4789-982d-595ae1be3f1d",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Paralelismo y Particiones\n",
    "El paralelismo en Spark se logra dividiendo los datos en particiones. Un DataFrame es una abstracci칩n sobre un RDD (Resilient Distributed Dataset), el cual es una colecci칩n de elementos particionados. Cada partici칩n puede ser procesada de forma independiente y en paralelo por un executor en un nodo del cl칰ster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38d9deb-26d3-43d9-8f12-d3cad265bf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido dividido en 12 particiones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===============================================>         (10 + 2) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuci칩n de registros en 12 particiones:\n",
      "[0, 0, 1048576, 0, 0, 0, 1048576, 0, 0, 0, 867472, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Obtener el n칰mero de particiones por defecto\n",
    "num_particiones = df_spark.rdd.getNumPartitions()\n",
    "print(f\"El DataFrame ha sido dividido en {num_particiones} particiones.\")\n",
    "\n",
    "# 2. Inspeccionar la distribuci칩n de registros por partici칩n\n",
    "# Se utiliza el RDD subyacente para aplicar una funci칩n a cada partici칩n.\n",
    "filas_por_particion = df_spark.rdd.mapPartitions(lambda iter: [sum(1 for _ in iter)]).collect()\n",
    "\n",
    "print(f\"\\nDistribuci칩n de registros en {len(filas_por_particion)} particiones:\")\n",
    "print(filas_por_particion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bcadd-ff3d-4a74-bd68-7b6ea44fca5c",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Modelo de Ejecuci칩n - Transformaciones y Acciones\n",
    "El modelo de programaci칩n de Spark se basa en la construcci칩n de un plan de ejecuci칩n a trav칠s de transformaciones, que culmina con la ejecuci칩n de dicho plan mediante una acci칩n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d321950-c60a-4417-ba8e-4e59d1c909fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plan de ejecuci칩n optimizado por Spark (DAG):\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [passenger_count#16L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(passenger_count#16L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=80]\n",
      "      +- HashAggregate(keys=[passenger_count#16L], functions=[avg(propina_pct#76), count(1)])\n",
      "         +- Exchange hashpartitioning(passenger_count#16L, 200), ENSURE_REQUIREMENTS, [plan_id=77]\n",
      "            +- HashAggregate(keys=[passenger_count#16L], functions=[partial_avg(propina_pct#76), partial_count(1)])\n",
      "               +- Project [passenger_count#16L, ((tip_amount#26 / total_amount#29) * 100.0) AS propina_pct#76]\n",
      "                  +- Filter (((isnotnull(passenger_count#16L) AND isnotnull(trip_distance#17)) AND (passenger_count#16L > 1)) AND (trip_distance#17 > 5.0))\n",
      "                     +- FileScan parquet [passenger_count#16L,trip_distance#17,tip_amount#26,total_amount#29] Batched: true, DataFilters: [isnotnull(passenger_count#16L), isnotnull(trip_distance#17), (passenger_count#16L > 1), (trip_di..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/app/notebooks/data/yellow_tripdata_2024-01.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(passenger_count), IsNotNull(trip_distance), GreaterThan(passenger_count,1), GreaterTha..., ReadSchema: struct<passenger_count:bigint,trip_distance:double,tip_amount:double,total_amount:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Definici칩n del Plan L칩gico (Transformaciones) ---\n",
    "# Estas operaciones son perezosas y construyen un Grafo Ac칤clico Dirigido (DAG).\n",
    "\n",
    "# Filtro de viajes con m치s de 1 pasajero y distancia superior a 5 millas\n",
    "df_filtrado = df_spark.filter(\n",
    "    (F.col(\"passenger_count\") > 1) &\n",
    "    (F.col(\"trip_distance\") > 5)\n",
    ")\n",
    "\n",
    "# Creaci칩n de una nueva columna con el porcentaje de propina\n",
    "df_con_propina_pct = df_filtrado.withColumn(\n",
    "    \"propina_pct\",\n",
    "    (F.col(\"tip_amount\") / F.col(\"total_amount\")) * 100\n",
    ")\n",
    "\n",
    "# Agregaci칩n para calcular la propina promedio por n칰mero de pasajeros\n",
    "df_agrupado = df_con_propina_pct.groupBy(\"passenger_count\").agg(\n",
    "    F.avg(\"propina_pct\").alias(\"propina_promedio_pct\"),\n",
    "    F.count(\"*\").alias(\"conteo_viajes\")\n",
    ")\n",
    "\n",
    "# Ordenamiento del resultado\n",
    "df_plan_final = df_agrupado.orderBy(F.col(\"passenger_count\").desc())\n",
    "\n",
    "# --- 2. Inspecci칩n del Plan F칤sico de Ejecuci칩n ---\n",
    "print(\"Plan de ejecuci칩n optimizado por Spark (DAG):\")\n",
    "df_plan_final.explain()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5a02d-eae4-4b96-aee6-f75b6313e4cb",
   "metadata": {},
   "source": [
    "## Ejercicio 5: Ejecuci칩n del Plan y la Operaci칩n de Shuffle\n",
    "Finalmente, una acci칩n como .collect() o .show() desencadena la ejecuci칩n del DAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185de3f6-d3de-4233-9198-0954a912852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ejecutando el plan de computaci칩n distribuida... ---\n",
      "CPU times: user 1.81 ms, sys: 1.18 ms, total: 3 ms\n",
      "Wall time: 365 ms\n",
      "\n",
      "--- Resultados del An치lisis ---\n",
      "Pasajeros: 8, \tConteo: 5, \tPropina Promedio: 11.98%\n",
      "Pasajeros: 7, \tConteo: 2, \tPropina Promedio: 18.32%\n",
      "Pasajeros: 6, \tConteo: 2917, \tPropina Promedio: 11.75%\n",
      "Pasajeros: 5, \tConteo: 4624, \tPropina Promedio: 11.64%\n",
      "Pasajeros: 4, \tConteo: 10183, \tPropina Promedio: 9.57%\n",
      "Pasajeros: 3, \tConteo: 16344, \tPropina Promedio: 10.46%\n",
      "Pasajeros: 2, \tConteo: 74759, \tPropina Promedio: 11.18%\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Ejecuci칩n del Plan (Acci칩n) ---\n",
    "# La acci칩n .collect() transfiere los resultados al nodo driver.\n",
    "# ADVERTENCIA: Usar .collect() solo con resultados de tama침o manejable.\n",
    "print(\"\\n--- Ejecutando el plan de computaci칩n distribuida... ---\")\n",
    "\n",
    "%time resultados = df_plan_final.collect()\n",
    "\n",
    "print(\"\\n--- Resultados del An치lisis ---\")\n",
    "for fila in resultados:\n",
    "    print(f\"Pasajeros: {fila['passenger_count']}, \\tConteo: {fila['conteo_viajes']}, \\tPropina Promedio: {fila['propina_promedio_pct']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178b0c2-bb71-4db4-a546-26609b722084",
   "metadata": {},
   "source": [
    "## Clase 2\n",
    "\n",
    "Continuamos estudiando Spark. Observa atentamente los tiempos de ejecuci칩n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fd0d2f-49ca-47ed-a8f2-512a32e68c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el DataFrame en Spark...\n",
      "data/yellow_tripdata_2024-01.parquet\n",
      "CPU times: user 545 풮s, sys: 662 풮s, total: 1.21 ms\n",
      "Wall time: 34.8 ms\n",
      "\n",
      "--- Ejecutando una 'Acci칩n' (como .count()) ---\n",
      "CPU times: user 132 풮s, sys: 0 ns, total: 132 풮s\n",
      "Wall time: 56.6 ms\n",
      "El DataFrame de Spark contiene 2964624 registros.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando el DataFrame en Spark...\")\n",
    "print(filepath)\n",
    "%time df_spark = spark.read.parquet(filepath)\n",
    "\n",
    "print(\"\\n--- Ejecutando una 'Acci칩n' (como .count()) ---\")\n",
    "%time total_filas = df_spark.count()\n",
    "\n",
    "print(f\"El DataFrame de Spark contiene {total_filas} registros.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd806eb1-1460-4054-9654-9c2bfa96e03b",
   "metadata": {},
   "source": [
    "**An치lisis y Filosof칤a de Spark:**\n",
    "\n",
    "* `spark.read.parquet(filepath)`: 춰El tiempo de ejecuci칩n fue casi **cero** (milisegundos)!\n",
    "* **쯇or Qu칠?** Porque Spark no carg칩 los datos. Esto es la **Evaluaci칩n Perezosa (Lazy Evaluation)**.\n",
    "* **Transformaciones (La Receta):** `spark.read.parquet` es una **Transformaci칩n**. Es una instrucci칩n, una \"promesa\" de que *eventualmente* leer치s ese archivo. Spark simplemente anota en su plan: \"OK, empezar칠 por este archivo\".\n",
    "* **Acciones (La Orden):** `.count()` es una **Acci칩n**. Es la orden de \"춰Ejecuta el plan y dame un resultado!\". Solo cuando pides una acci칩n, Spark realmente empieza a trabajar.\n",
    "* **La Ventaja:** Esto permite a Spark construir un plan de ejecuci칩n complejo y optimizarlo *antes* de mover un solo byte de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e303e52-0fe9-46fb-8c89-62a27f0f0d9a",
   "metadata": {},
   "source": [
    "## Ejercicio 쮺칩mo procesa Spark?\n",
    "쮺칩mo procesa Spark los datos sin cargarlos todos a la RAM? Dividi칠ndolos en **Particiones**. Una partici칩n es un \"trozo\" del conjunto de datos que puede ser procesado por un trabajador (un n칰cleo de CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa56039-68f0-4ee7-9508-bfe17724461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El DataFrame ha sido dividido en 12 particiones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==============================================>         (10 + 2) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuci칩n de registros en 12 particiones:\n",
      "[0, 0, 1048576, 0, 0, 0, 1048576, 0, 0, 0, 867472, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. 쮺u치ntas particiones (lotes) cre칩 Spark?\n",
    "num_particiones = df_spark.rdd.getNumPartitions()\n",
    "print(f\"El DataFrame ha sido dividido en {num_particiones} particiones.\")\n",
    "\n",
    "# 2. Miremos DENTRO de las particiones (쯖u치ntas filas tiene cada una?)\n",
    "filas_por_particion = df_spark.rdd.mapPartitions(lambda iter: [sum(1 for _ in iter)]).collect()\n",
    "print(f\"\\nDistribuci칩n de registros en {len(filas_por_particion)} particiones:\")\n",
    "print(filas_por_particion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749f029-c87d-4eee-88e4-bddac4b2890c",
   "metadata": {},
   "source": [
    "**Salida Esperada (춰Nuestra Primera Pista!):**\n",
    "\n",
    "El DataFrame ha sido dividido en 12 particiones. Distribuci칩n de registros en 12 particiones: [0, 0, 1048576, 0, 0, 0, 1048576, 0, 0, 0, 867472, 0]\n",
    "\n",
    "\n",
    "**An치lisis y Filosof칤a de Spark:**\n",
    "\n",
    "* `df_spark.rdd.getNumPartitions()`: `rdd` es la API de bajo nivel de Spark. Le estamos preguntando: \"쮼n cu치ntos lotes se dividi칩 el trabajo?\".\n",
    "* **Grado de Paralelismo:** Responde 12. Esto es porque nuestro `.master(\"local[*]\")` detect칩 12 n칰cleos de CPU, por lo que 12 es el n칰mero de hilos \"trabajadores\" por defecto.\n",
    "* **Desbalanceo de Datos (Data Skew):** 춰Este es el punto clave! La lista `[0, 0, 1048576, ...]` nos dice que **9 de nuestros 12 trabajadores no est치n haciendo nada**. Todo el trabajo de lectura se concentra en solo 3 particiones (trabajadores).\n",
    "* **쯇or qu칠?** No es un error. Es un artefacto de la lectura. El archivo Parquet que descargamos est치 almacenado en el disco en 3 \"Row Groups\" (bloques) principales. Spark, al leerlo, asign칩 cada bloque a una partici칩n.\n",
    "* **Conclusi칩n:** Aunque tenemos 12 n칰cleos listos, en la primera etapa del trabajo, solo 3 est치n activos. **Lo arreglaremos en lo que sigue.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21c396-9a55-4730-9b63-f4d7a73b888d",
   "metadata": {},
   "source": [
    "**An치lisis y Filosof칤a de Spark:**\n",
    "\n",
    "* `df_spark.rdd.getNumPartitions()`: `rdd` es la API de bajo nivel de Spark. Le estamos preguntando: \"쮼n cu치ntos lotes se dividi칩 el trabajo?\".\n",
    "* **Grado de Paralelismo:** Responde 12. Esto es porque nuestro `.master(\"local[*]\")` detect칩 12 n칰cleos de CPU, por lo que 12 es el n칰mero de hilos \"trabajadores\" por defecto.\n",
    "* **Desbalanceo de Datos (Data Skew):** 춰Este es el punto clave! La lista `[0, 0, 1048576, ...]` nos dice que **9 de nuestros 12 trabajadores no est치n haciendo nada**. Todo el trabajo de lectura se concentra en solo 3 particiones (trabajadores).\n",
    "* **쯇or qu칠?** No es un error. Es un artefacto de la lectura. El archivo Parquet que descargamos est치 almacenado en el disco en 3 \"Row Groups\" (bloques) principales. Spark, al leerlo, asign칩 cada bloque a una partici칩n.\n",
    "* **Conclusi칩n:** Aunque tenemos 12 n칰cleos listos, en la primera etapa del trabajo, solo 3 est치n activos. **Lo arreglaremos en el Ejercicio 6.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15badb0b-3876-4bce-a431-49517122f6bd",
   "metadata": {},
   "source": [
    "## Ejercicio: Optimizando - Uso Completo de Recursos\n",
    "\n",
    "Solo 3 de nuestros 12 n칰cleos estaban trabajando en la lectura. Vamos a arreglar esto.\n",
    "\n",
    "**La Soluci칩n:** `repartition()`\n",
    "Le daremos a Spark una orden expl칤cita para que baraje los datos *inmediatamente* despu칠s de leerlos y los redistribuya equitativamente en 12 particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e92ad9-2c62-404a-b726-a9aa567ef9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particiones originales: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==========================================>              (9 + 3) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas particiones: 12\n",
      "\n",
      "Verificando la nueva distribuci칩n (esto tomar치 un momento)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                       (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuci칩n de registros en 12 particiones (despu칠s de repartition):\n",
      "[247051, 247051, 247051, 247052, 247052, 247053, 247054, 247053, 247053, 247052, 247051, 247051]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp_ntz, tpep_dropoff_datetime: timestamp_ntz, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, Airport_fee: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Creamos un nuevo DataFrame, forzando la redistribuci칩n\n",
    "print(f\"Particiones originales: {df_spark.rdd.getNumPartitions()}\")\n",
    "df_reparticionado = df_spark.repartition(12)\n",
    "\n",
    "# 2. Verifiquemos la nueva distribuci칩n\n",
    "# .cache() es una transformaci칩n que le dice a Spark \"guarda el resultado de\n",
    "# esta repartici칩n en memoria para que no tengamos que hacerla de nuevo\".\n",
    "df_reparticionado.cache()\n",
    "print(f\"Nuevas particiones: {df_reparticionado.rdd.getNumPartitions()}\")\n",
    "print(\"\\nVerificando la nueva distribuci칩n (esto tomar치 un momento)...\")\n",
    "\n",
    "# Esta acci칩n (.count()) fuerza la ejecuci칩n del .repartition() y .cache()\n",
    "df_reparticionado.count() \n",
    "\n",
    "# Ahora miremos dentro de las particiones cacheadas\n",
    "nuevas_filas_por_particion = df_reparticionado.rdd.mapPartitions(lambda iter: [sum(1 for _ in iter)]).collect()\n",
    "print(f\"Distribuci칩n de registros en 12 particiones (despu칠s de repartition):\")\n",
    "print(nuevas_filas_por_particion)\n",
    "\n",
    "# Liberar la memoria cacheada\n",
    "df_reparticionado.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc90c81-f942-4776-8922-a13cca5b9b0c",
   "metadata": {},
   "source": [
    "**An치lisis y Filosof칤a de Spark:**\n",
    "\n",
    "* `df_spark.repartition(12)`: Es una **Transformaci칩n Ancha (Wide)**. Le dimos a Spark la orden expl칤cita de \"ejecutar un SHUFFLE completo y redistribuir todos los datos en 12 nuevas particiones de tama침o (casi) id칠ntico\".\n",
    "* **춰칄xito!** La nueva distribuci칩n de filas es perfectamente balanceada.\n",
    "* **La Ventaja:** Ahora, cualquier operaci칩n que hagamos (como nuestro `filter` o `project`) utilizar치 **los 12 n칰cleos en paralelo** desde el primer segundo.\n",
    "* **El Costo:** `repartition()` es en s칤 mismo un *shuffle* completo, 춰lo cual es costoso! Introdujimos un costo inicial, pero lo hicimos para que el resto del procesamiento sea mucho m치s r치pido y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b14e51-5b46-4015-9a30-8195917e5898",
   "metadata": {},
   "source": [
    "## Sint치xis de spark\n",
    "\n",
    "### Parte 1: Selecci칩n y Filtro (El `SELECT` y `WHERE` de SQL)\n",
    "\n",
    "Empecemos por lo b치sico: c칩mo seleccionar columnas y filtrar filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aeac506-8d14-40d2-bc4a-54c9aed0aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siempre importamos las funciones de SQL\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Carguemos nuestro DataFrame original (el desbalanceado est치 bien para esto)\n",
    "df_spark = spark.read.parquet(\"data/yellow_tripdata_2024-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb6c0-5c89-4e61-9a0e-2d1325a3ddec",
   "metadata": {},
   "source": [
    "La funci칩n m치s b치sica. Le dices qu칠 columnas quieres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "380ff47b-fdc1-4b1e-a2f4-1443dfd8e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame solo con 3 columnas:\n",
      "+---------------+-------------+------------+\n",
      "|passenger_count|trip_distance|total_amount|\n",
      "+---------------+-------------+------------+\n",
      "|              1|         1.72|        22.7|\n",
      "|              1|          1.8|       18.75|\n",
      "|              1|          4.7|        31.3|\n",
      "|              1|          1.4|        17.0|\n",
      "|              1|          0.8|        16.1|\n",
      "+---------------+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: SELECT passenger_count, trip_distance FROM ...\n",
    "df_seleccion = df_spark.select(\"passenger_count\", \"trip_distance\", \"total_amount\")\n",
    "\n",
    "print(\"DataFrame solo con 3 columnas:\")\n",
    "df_seleccion.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c5968-1e71-4dfc-8c8a-824472061064",
   "metadata": {},
   "source": [
    "`selectExpr` (Select Expression) te permite escribir pseudo-SQL dentro de una string de Python. Es muy 칰til para c치lculos r치pidos o para renombrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6ddf0d-a32c-48ba-8575-7fc8e7604892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con columnas calculadas y renombradas:\n",
      "+-------------+------------------+---------+\n",
      "|trip_distance|  trip_distance_km|pasajeros|\n",
      "+-------------+------------------+---------+\n",
      "|         1.72|         2.7680648|        1|\n",
      "|          1.8|          2.896812|        1|\n",
      "|          4.7|          7.563898|        1|\n",
      "|          1.4|2.2530759999999996|        1|\n",
      "|          0.8|1.2874720000000002|        1|\n",
      "+-------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: SELECT trip_distance, trip_distance * 1.60934 AS trip_distance_km FROM ...\n",
    "df_expr = df_spark.selectExpr(\n",
    "    \"trip_distance\", \n",
    "    \"trip_distance * 1.60934 AS trip_distance_km\",\n",
    "    \"passenger_count AS pasajeros\"\n",
    ")\n",
    "\n",
    "print(\"DataFrame con columnas calculadas y renombradas:\")\n",
    "df_expr.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698addc-559c-4dac-9af5-6c2d6c7fb7e1",
   "metadata": {},
   "source": [
    "**`filter()` y `F.col()` - El `WHERE` de SQL**\n",
    "\n",
    "Aqu칤 es donde entra la filosof칤a de `F.col()`.\n",
    "\n",
    "* `\"passenger_count\"` (string): Es solo el *nombre* de la columna.\n",
    "* `F.col(\"passenger_count\")` (objeto Columna): Es la *columna real* sobre la que podemos hacer operaciones (como `>`, `==`, `+`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b47bd5-d891-4ed2-9f9f-ee2f7363304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes largos con m치s de 1 pasajero:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:35:16|  2024-01-01 01:11:52|              2|          8.2|         1|                 N|         246|         190|           1|       59.0|  3.5|    0.5|     14.15|        6.94|                  1.0|       85.09|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:49:31|  2024-01-01 01:35:41|              2|         8.89|         1|                 N|          79|          41|           1|       47.8|  1.0|    0.5|      7.92|         0.0|                  1.0|       60.72|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:40:01|  2024-01-01 01:18:26|              2|         5.27|         1|                 N|         232|         246|           1|       38.0|  1.0|    0.5|       8.6|         0.0|                  1.0|        51.6|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:45:26|  2024-01-01 01:23:16|              2|         5.35|         1|                 N|          79|          48|           1|       34.5|  1.0|    0.5|      6.08|         0.0|                  1.0|       45.58|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: WHERE passenger_count > 1 AND trip_distance > 5\n",
    "df_filtrado = df_spark.filter(\n",
    "    (F.col(\"passenger_count\") > 1) & \n",
    "    (F.col(\"trip_distance\") > 5)\n",
    ")\n",
    "\n",
    "print(\"Viajes largos con m치s de 1 pasajero:\")\n",
    "df_filtrado.show(5)\n",
    "\n",
    "# Recordatorio de sintaxis:\n",
    "# En PySpark, usa '&' (AND), '|' (OR), y '~' (NOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7629325d-52b7-4637-9682-e69f8deebc73",
   "metadata": {},
   "source": [
    "**`where()` - El alias de `filter()`**\n",
    "\n",
    "`where()` hace *exactamente* lo mismo que `filter()`. Es solo un alias para que los que vienen de SQL se sientan m치s c칩modos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae297b21-0122-4f47-bfb5-a0de1f9a8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes pagados con Tarjeta de Cr칠dito (payment_type 1):\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: WHERE payment_type = 1\n",
    "df_where = df_spark.where(F.col(\"payment_type\") == 1)\n",
    "\n",
    "print(\"Viajes pagados con Tarjeta de Cr칠dito (payment_type 1):\")\n",
    "df_where.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85b9fc-2011-4bb8-afe5-b4217c01a1fc",
   "metadata": {},
   "source": [
    "**`like()` - B칰squeda de patrones**\n",
    "\n",
    "Al igual que en SQL, `like()` busca patrones en strings. `store_and_fwd_flag` es una columna con 'Y' o 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ff0c154-6cb3-435c-980d-cb268f2bd8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes con 'store_and_fwd_flag' = Y:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       1| 2024-01-01 00:51:58|  2024-01-01 01:05:44|              2|          2.9|         1|                 Y|         264|         264|           1|       16.3|  3.5|    0.5|      4.25|         0.0|                  1.0|       25.55|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:35:56|  2024-01-01 00:56:40|              2|          2.6|         1|                 Y|         239|         162|           2|       17.0|  3.5|    0.5|       0.0|         0.0|                  1.0|        22.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:29|  2024-01-01 01:24:52|              1|          2.6|         1|                 Y|          50|         170|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:11:26|  2024-01-01 00:32:29|              1|         2.21|         1|                 Y|         158|         186|           1|       19.8|  1.0|    0.5|       1.0|         0.0|                  1.0|        25.8|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:27:01|  2024-01-01 00:38:01|              1|          1.5|         1|                 Y|         234|         246|           1|       12.1|  3.5|    0.5|       5.1|         0.0|                  1.0|        22.2|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:08:07|  2024-01-01 00:14:19|              1|          1.1|         1|                 Y|         141|         262|           1|        8.6|  3.5|    0.5|       3.4|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:53:06|  2024-01-01 01:26:22|              1|          5.0|         1|                 Y|         263|         249|           1|       32.4|  3.5|    0.5|      7.45|         0.0|                  1.0|       44.85|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:04:52|  2024-01-01 00:30:01|              1|          3.1|         1|                 Y|          48|         142|           1|       24.0|  3.5|    0.5|       5.8|         0.0|                  1.0|        34.8|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:34:26|  2024-01-01 00:46:10|              1|          1.8|         1|                 Y|         142|         140|           1|       13.5|  3.5|    0.5|       5.5|         0.0|                  1.0|        24.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:51:34|  2024-01-01 00:56:01|              1|          0.5|         1|                 Y|         141|         236|           4|        5.8|  3.5|    0.5|       0.0|         0.0|                  1.0|        10.8|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL: WHERE store_and_fwd_flag LIKE 'Y'\n",
    "df_like = df_spark.filter( F.col(\"store_and_fwd_flag\").like(\"Y\") )\n",
    "\n",
    "print(f\"Viajes con 'store_and_fwd_flag' = Y:\")\n",
    "df_like.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca706e-713c-4328-91e3-08656e511d43",
   "metadata": {},
   "source": [
    "**`isin()` - M칰ltiples `OR`**\n",
    "\n",
    "`isin()` es un atajo para `WHERE mi_columna = 'A' OR mi_columna = 'B' ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ab7282d-b1aa-41be-a92e-fefbb179f58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes pagados con Tarjeta o Efectivo:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Busquemos viajes pagados con \"Tarjeta\" (1) o \"Efectivo\" (2)\n",
    "df_isin = df_spark.filter( F.col(\"payment_type\").isin(1, 2) )\n",
    "\n",
    "print(\"Viajes pagados con Tarjeta o Efectivo:\")\n",
    "df_isin.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bcccc-c568-4571-af26-43cf9eefcf5d",
   "metadata": {},
   "source": [
    "### Parte 2: Manipulaci칩n de Columnas (Ingenier칤a de Caracter칤sticas)\n",
    "\n",
    "Aqu칤 es donde Spark brilla. Crear nuevas columnas es la base de la ingenier칤a de caracter칤sticas (feature engineering)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f7d73-577d-4a99-8e0c-1c50dbac6d4a",
   "metadata": {},
   "source": [
    "**`withColumn()` - La funci칩n m치s importante**\n",
    "\n",
    "`withColumn(\"nombre_columna\", <operaci칩n>)` es la forma est치ndar de crear una nueva columna o reemplazar una existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a731775-59e4-48cf-b5ce-a9a1dd600e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con 'propina_pct' (puede ser NaN si total_amount es 0):\n",
      "+--------+----------+------------+------------------+---------------+\n",
      "|VendorID|tip_amount|total_amount|       propina_pct|passenger_count|\n",
      "+--------+----------+------------+------------------+---------------+\n",
      "|       2|       0.0|        22.7|               0.0|              1|\n",
      "|       1|      3.75|       18.75|              20.0|              1|\n",
      "|       1|       3.0|        31.3| 9.584664536741213|              1|\n",
      "|       1|       2.0|        17.0| 11.76470588235294|              1|\n",
      "|       1|       3.2|        16.1|19.875776397515526|              1|\n",
      "+--------+----------+------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos a crear la columna 'propina_pct' que usamos en la clase anterior\n",
    "df_con_columna = df_spark.withColumn(\n",
    "    \"propina_pct\",\n",
    "    (F.col(\"tip_amount\") / F.col(\"total_amount\")) * 100\n",
    ")\n",
    "\n",
    "print(\"DataFrame con 'propina_pct' (puede ser NaN si total_amount es 0):\")\n",
    "df_con_columna.select(\"VendorID\",\"tip_amount\", \"total_amount\", \"propina_pct\",\"passenger_count\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ecfa1b-033a-4294-801f-33978cb0e6a7",
   "metadata": {},
   "source": [
    "**`F.lit()` - Introduciendo un Valor Literal**\n",
    "\n",
    "쯈u칠 pasa si quieres a침adir una columna donde *todas* las filas tengan el mismo valor (ej. \"Hola\", o el n칰mero 100)?\n",
    "\n",
    "No puedes solo escribir `\"Hola\"`, porque Spark pensar치 que es el *nombre* de una columna. Debes usar `F.lit()` (literal) para decirle \"este es un valor constante, no el nombre de una columna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d0f331-8676-475b-a90b-0856a9690856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|passenger_count|  fuente|\n",
      "+---------------+--------+\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "|              1|taxis_ny|\n",
      "+---------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A침adimos una columna 'fuente' con el valor constante 'taxis_ny'\n",
    "df_con_literal = df_spark.withColumn(\"fuente\", F.lit(\"taxis_ny\"))\n",
    "\n",
    "df_con_literal.select(\"passenger_count\", \"fuente\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf41f0-6654-4c6f-882c-f178425982f4",
   "metadata": {},
   "source": [
    "**`withColumnRenamed()` y `drop()` - Limpieza**\n",
    "\n",
    "Funciones b치sicas de limpieza: renombrar y eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6033e8d5-2691-4e35-abc1-e2080408996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615020cb-b63c-4df9-9d46-c78340a4decb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas despu칠s de renombrar y eliminar:\n",
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- pasajeros: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renombremos 'passenger_count' a 'pasajeros' y eliminemos 'store_and_fwd_flag'\n",
    "df_limpio = df_spark.withColumnRenamed(\"passenger_count\", \"pasajeros\") \\\n",
    "                    .drop(\"store_and_fwd_flag\")\n",
    "\n",
    "print(\"Columnas despu칠s de renombrar y eliminar:\")\n",
    "df_limpio.printSchema() # .printSchema() nos muestra la estructura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c666e4-df31-46b4-a161-fffff396a9db",
   "metadata": {},
   "source": [
    "### Parte 3: El Poder de `F` (L칩gica, Fechas y Nulos)\n",
    "\n",
    "Aqu칤 es donde `pyspark.sql.functions` (nuestro `F`) realmente muestra su poder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9b10c-50d5-4e92-980d-cecc3315c0cf",
   "metadata": {},
   "source": [
    "**`F.when()` - El `CASE WHEN` de SQL**\n",
    "\n",
    "Esta es *extremadamente* poderosa. Te permite crear l칩gica condicional (if/then/else) para una nueva columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c154e68f-4a0a-4c98-a5b2-346bce2027b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con columna condicional 'tipo_viaje':\n",
      "+-------------+---------------+\n",
      "|trip_distance|     tipo_viaje|\n",
      "+-------------+---------------+\n",
      "|         1.72|          Corto|\n",
      "|          1.8|          Corto|\n",
      "|          4.7|          Corto|\n",
      "|          1.4|          Corto|\n",
      "|          0.8|          Corto|\n",
      "|          4.7|          Corto|\n",
      "|        10.82|Largo (No Aero)|\n",
      "|          3.0|          Corto|\n",
      "|         5.44|          Medio|\n",
      "|         0.04|          Corto|\n",
      "|         0.75|          Corto|\n",
      "|          1.2|          Corto|\n",
      "|          8.2|          Medio|\n",
      "|          0.4|          Corto|\n",
      "|          0.8|          Corto|\n",
      "|          5.0|          Corto|\n",
      "|          1.5|          Corto|\n",
      "|          0.0|          Corto|\n",
      "|          1.5|          Corto|\n",
      "|         2.57|          Corto|\n",
      "+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creemos una columna \"tipo_viaje\" basada en la distancia\n",
    "df_con_when = df_spark.withColumn(\"tipo_viaje\",\n",
    "    F.when(F.col(\"trip_distance\") > 20, F.lit(\"Largo (Aero)\"))\n",
    "        .when(F.col(\"trip_distance\") > 10, F.lit(\"Largo (No Aero)\"))\n",
    "     .when(F.col(\"trip_distance\") > 5, F.lit(\"Medio\"))\n",
    "     .otherwise(F.lit(\"Corto\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame con columna condicional 'tipo_viaje':\")\n",
    "df_con_when.select(\"trip_distance\", \"tipo_viaje\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f1608-0e53-4164-a871-65b6f1b193f6",
   "metadata": {},
   "source": [
    "**Manejo de Nulos ( `isNull`, `isNotNull`, `fillna` )**\n",
    "\n",
    "Los datos del mundo real est치n sucios. `passenger_count` puede tener nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c57543-84a1-42ce-aad5-e414dd91804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes con 'passenger_count' nulo: 140162\n"
     ]
    }
   ],
   "source": [
    "# Contemos cu치ntos nulos hay\n",
    "conteo_nulos = df_spark.filter(F.col(\"passenger_count\").isNull()).count()\n",
    "print(f\"Viajes con 'passenger_count' nulo: {conteo_nulos}\")\n",
    "\n",
    "# Usemos fillna() para reemplazar nulos con un valor (ej. 1 pasajero por defecto)\n",
    "# .fillna() es un m칠todo del DataFrame, no de F.\n",
    "df_sin_nulos = df_spark.fillna(1, subset=[\"passenger_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79f93b66-9608-4350-a2c2-d01d7783085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viajes con 'passenger_count' nulo en df_sin_nulos: 0\n"
     ]
    }
   ],
   "source": [
    "df_sin_nulos = df_spark.fillna(1, subset=[\"passenger_count\"])\n",
    "conteo_nulos = df_sin_nulos.filter(F.col(\"passenger_count\").isNull()).count()\n",
    "print(f\"Viajes con 'passenger_count' nulo en df_sin_nulos: {conteo_nulos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a23f25-ac74-469c-9a73-717d1a9d4891",
   "metadata": {},
   "source": [
    "**Funciones de Fecha/Hora (`year`, `month`, `dayofweek`)**\n",
    "\n",
    "Nuestras columnas `tpep_pickup_datetime` son *timestamps*. Podemos extraer sus componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a39ab15a-9240-4f09-8b21-cbea2a89523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con componentes de fecha/hora extra칤dos:\n",
      "+--------------------+----+---+----------+----+\n",
      "|tpep_pickup_datetime| a침o|mes|dia_semana|hora|\n",
      "+--------------------+----+---+----------+----+\n",
      "| 2024-01-01 00:57:55|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:03:00|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:17:06|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:36:38|2024|  1|         2|   0|\n",
      "| 2024-01-01 00:46:51|2024|  1|         2|   0|\n",
      "+--------------------+----+---+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extraigamos el a침o, mes, d칤a y hora de recogida\n",
    "df_con_fechas = df_spark.withColumn(\"a침o\", F.year(F.col(\"tpep_pickup_datetime\"))) \\\n",
    "                        .withColumn(\"mes\", F.month(F.col(\"tpep_pickup_datetime\"))) \\\n",
    "                        .withColumn(\"dia_semana\", F.dayofweek(F.col(\"tpep_pickup_datetime\"))) \\\n",
    "                        .withColumn(\"hora\", F.hour(F.col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "print(\"DataFrame con componentes de fecha/hora extra칤dos:\")\n",
    "df_con_fechas.select(\"tpep_pickup_datetime\", \"a침o\", \"mes\", \"dia_semana\", \"hora\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85568ca0-0847-4cff-85b0-8f1a0555991f",
   "metadata": {},
   "source": [
    "**C치lculo de Duraci칩n**\n",
    "\n",
    "Podemos hacer aritm칠tica con fechas. Calculemos la duraci칩n del viaje en segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fccab3bc-a57d-45e7-beb9-036e156cdf24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION] Cannot resolve \"CAST(tpep_dropoff_datetime AS BIGINT)\" due to data type mismatch: cannot cast \"TIMESTAMP_NTZ\" to \"BIGINT\".;\n'Project [VendorID#1174, tpep_pickup_datetime#1175, tpep_dropoff_datetime#1176, passenger_count#1177L, trip_distance#1178, RatecodeID#1179L, store_and_fwd_flag#1180, PULocationID#1181, DOLocationID#1182, payment_type#1183L, fare_amount#1184, extra#1185, mta_tax#1186, tip_amount#1187, tolls_amount#1188, improvement_surcharge#1189, total_amount#1190, congestion_surcharge#1191, Airport_fee#1192, (cast(tpep_dropoff_datetime#1176 as bigint) - cast(tpep_pickup_datetime#1175 as bigint)) AS duracion_segundos#1946]\n+- Relation [VendorID#1174,tpep_pickup_datetime#1175,tpep_dropoff_datetime#1176,passenger_count#1177L,trip_distance#1178,RatecodeID#1179L,store_and_fwd_flag#1180,PULocationID#1181,DOLocationID#1182,payment_type#1183L,fare_amount#1184,extra#1185,mta_tax#1186,tip_amount#1187,tolls_amount#1188,improvement_surcharge#1189,total_amount#1190,congestion_surcharge#1191,Airport_fee#1192] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Restar timestamps nos da un \"Intervalo\". \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Necesitamos convertirlo a segundos usando .cast(\"long\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_con_duracion = \u001b[43mdf_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mduracion_segundos\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtpep_dropoff_datetime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlong\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtpep_pickup_datetime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlong\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df_con_duracion.select(\u001b[33m\"\u001b[39m\u001b[33mduracion_segundos\u001b[39m\u001b[33m\"\u001b[39m).show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5174\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   5169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   5170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   5171\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5172\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   5173\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m5174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION] Cannot resolve \"CAST(tpep_dropoff_datetime AS BIGINT)\" due to data type mismatch: cannot cast \"TIMESTAMP_NTZ\" to \"BIGINT\".;\n'Project [VendorID#1174, tpep_pickup_datetime#1175, tpep_dropoff_datetime#1176, passenger_count#1177L, trip_distance#1178, RatecodeID#1179L, store_and_fwd_flag#1180, PULocationID#1181, DOLocationID#1182, payment_type#1183L, fare_amount#1184, extra#1185, mta_tax#1186, tip_amount#1187, tolls_amount#1188, improvement_surcharge#1189, total_amount#1190, congestion_surcharge#1191, Airport_fee#1192, (cast(tpep_dropoff_datetime#1176 as bigint) - cast(tpep_pickup_datetime#1175 as bigint)) AS duracion_segundos#1946]\n+- Relation [VendorID#1174,tpep_pickup_datetime#1175,tpep_dropoff_datetime#1176,passenger_count#1177L,trip_distance#1178,RatecodeID#1179L,store_and_fwd_flag#1180,PULocationID#1181,DOLocationID#1182,payment_type#1183L,fare_amount#1184,extra#1185,mta_tax#1186,tip_amount#1187,tolls_amount#1188,improvement_surcharge#1189,total_amount#1190,congestion_surcharge#1191,Airport_fee#1192] parquet\n"
     ]
    }
   ],
   "source": [
    "# Restar timestamps nos da un \"Intervalo\". \n",
    "# Necesitamos convertirlo a segundos usando .cast(\"long\")\n",
    "df_con_duracion = df_spark.withColumn(\"duracion_segundos\",\n",
    "    (F.col(\"tpep_dropoff_datetime\").cast(\"long\") - F.col(\"tpep_pickup_datetime\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "df_con_duracion.select(\"duracion_segundos\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe1748-94e3-4ec4-9ed7-8227cb8b578b",
   "metadata": {},
   "source": [
    "En la Celda anterior, te encontraste con un `AnalysisException`. 춰Felicidades! Este es tu primer error de *tipos de datos* en Spark.\n",
    "\n",
    "**El Error:** `AnalysisException: [DATATYPE_MISMATCH.CAST_WITHOUT_SUGGESTION] ... cannot cast \"TIMESTAMP_NTZ\" to \"BIGINT\"`\n",
    "\n",
    "**An치lisis del Error:**\n",
    "* El error nos dice que Spark no sabe c칩mo convertir un `TIMESTAMP_NTZ` (un *timestamp* o marca de tiempo, sin zona horaria) directamente a un `BIGINT` (un n칰mero entero largo, como `long`).\n",
    "* Tu c칩digo `F.col(...).cast(\"long\")` es lo que gener칩 este error.\n",
    "* En versiones antiguas de Spark, esto a veces funcionaba, ya que `cast(\"long\")` era un atajo para \"dame los segundos *epoch*\". En las versiones modernas, Spark es mucho m치s estricto con los tipos de datos para evitar ambig칲edades. No sabe si quieres los segundos, los milisegundos o los microsegundos.\n",
    "\n",
    "**La Soluci칩n:**\n",
    "Para obtener la duraci칩n en segundos, primero debemos convertir el *timestamp* a un tipo num칠rico que represente los segundos *epoch*. El tipo de dato correcto para esto es **`double`** (un n칰mero de punto flotante de alta precisi칩n).\n",
    "\n",
    "Aqu칤 est치 la celda corregida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f148cd74-46fa-422a-a401-f4f547881383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|New DropOff|tpep_dropoff_datetime|\n",
      "+-----------+---------------------+\n",
      "| 1704071863|  2024-01-01 01:17:43|\n",
      "| 1704067776|  2024-01-01 00:09:36|\n",
      "| 1704069301|  2024-01-01 00:35:01|\n",
      "| 1704069896|  2024-01-01 00:44:56|\n",
      "| 1704070377|  2024-01-01 00:52:57|\n",
      "+-----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_review = df_spark.withColumn(\"New DropOff\",F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")))\n",
    "df_time_review.select(\"New DropOff\",\"tpep_dropoff_datetime\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ef505f0-1482-425f-931d-1b6706f006e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- New DropOff: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6b35197-bc4a-4432-87a0-1e2cfa918667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C치lculo de duraci칩n exitoso:\n",
      "+--------------------+---------------------+-----------------+------------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|duracion_segundos|  duracion_minutos|\n",
      "+--------------------+---------------------+-----------------+------------------+\n",
      "| 2024-01-01 00:57:55|  2024-01-01 01:17:43|             1188|              19.8|\n",
      "| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              396|               6.6|\n",
      "| 2024-01-01 00:17:06|  2024-01-01 00:35:01|             1075|17.916666666666668|\n",
      "| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              498|               8.3|\n",
      "| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              366|               6.1|\n",
      "+--------------------+---------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Viajes con duraci칩n negativa o cero: 870\n"
     ]
    }
   ],
   "source": [
    "# Celda 13 (Corregida): C치lculo de Duraci칩n\n",
    "#\n",
    "# Corregimos el error anterior. En lugar de .cast(), usamos\n",
    "# la funci칩n expl칤cita F.unix_timestamp() para convertir\n",
    "# el timestamp en un n칰mero (segundos epoch).\n",
    "\n",
    "df_con_duracion = df_spark.withColumn(\"duracion_segundos\",\n",
    "    (F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")) - F.unix_timestamp(F.col(\"tpep_pickup_datetime\")))\n",
    ")\n",
    "\n",
    "# Ahora podemos hacer aritm칠tica simple\n",
    "df_con_duracion = df_con_duracion.withColumn(\"duracion_minutos\",\n",
    "    (F.col(\"duracion_segundos\") / 60).cast(\"double\") # Hacemos cast del *resultado*\n",
    ")\n",
    "\n",
    "print(\"C치lculo de duraci칩n exitoso:\")\n",
    "df_con_duracion.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"duracion_segundos\", \"duracion_minutos\").show(5)\n",
    "\n",
    "# Limpiemos los viajes con duraci칩n negativa (errores de datos)\n",
    "print(f\"Viajes con duraci칩n negativa o cero: {df_con_duracion.filter(F.col('duracion_segundos') <= 0).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b4e1b-4982-4738-aca5-038b4681da71",
   "metadata": {},
   "source": [
    "**Alternativa - `F.datediff()`**\n",
    "\n",
    "Si solo te importara la diferencia en *d칤as* (no segundos), podr칤as usar `datediff`. Nota que esta funci칩n opera sobre *fechas* (`date`), no sobre *marcas de tiempo* (`timestamp`), as칤 que primero debemos hacer un `cast(\"date\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "660fb0f4-9bab-48f1-85c7-11de6bad305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diferencia en D칈AS:\n",
      "+--------------------+---------------------+-------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|dias_de_viaje|\n",
      "+--------------------+---------------------+-------------+\n",
      "| 2024-01-01 00:57:55|  2024-01-01 01:17:43|            0|\n",
      "| 2024-01-01 00:03:00|  2024-01-01 00:09:36|            0|\n",
      "| 2024-01-01 00:17:06|  2024-01-01 00:35:01|            0|\n",
      "| 2024-01-01 00:36:38|  2024-01-01 00:44:56|            0|\n",
      "| 2024-01-01 00:46:51|  2024-01-01 00:52:57|            0|\n",
      "+--------------------+---------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dias = df_spark.withColumn(\"dias_de_viaje\",\n",
    "    F.datediff(\n",
    "        F.col(\"tpep_dropoff_datetime\").cast(\"date\"),\n",
    "        F.col(\"tpep_pickup_datetime\").cast(\"date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nDiferencia en D칈AS:\")\n",
    "df_dias.select(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"dias_de_viaje\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3ba81-92f8-493a-8829-11f23817e88e",
   "metadata": {},
   "source": [
    "### Parte 4: Agregaciones (El `groupBy` y `agg`)\n",
    "\n",
    "Aqu칤 volvemos al `groupBy`, pero con m치s funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e28b0d-a56c-48f6-9565-647099b570d8",
   "metadata": {},
   "source": [
    "**`groupBy()` con M칰ltiples Agregaciones**\n",
    "\n",
    "En lugar de solo `avg`, podemos pedirle a Spark que calcule todo a la vez: `avg`, `sum`, `max`, `min`, `count`, `countDistinct` (conteo de valores 칰nicos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "532d65b1-52ac-4301-8e2f-9a62e4c9a618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agregaci칩n completa por Zona de Recogida:\n",
      "+------------+-------------+------------------+--------------------+----------------+-----------------+\n",
      "|PULocationID|conteo_viajes|    promedio_total|          suma_total|distancia_maxima|tipos_pago_unicos|\n",
      "+------------+-------------+------------------+--------------------+----------------+-----------------+\n",
      "|         132|       145240| 76.57620855134614|1.1121928529997513E7|        10879.28|                5|\n",
      "|         161|       143471|23.482411149291384|   3369045.009999984|        38202.66|                5|\n",
      "|         237|       142708|19.453676878661152|  2776195.3199999756|           971.8|                5|\n",
      "|         236|       136465| 20.00189477155298|  2729558.5699999775|           58.81|                5|\n",
      "|         162|       106717| 22.88040002998577|   2441727.649999991|           71.18|                5|\n",
      "|         230|       106324| 26.26924946390265|  2793051.6799999853|            80.0|                5|\n",
      "|         186|       104523|23.640956153191134|   2471023.659999997|            94.0|                5|\n",
      "|         142|       104080|20.998902863182096|  2185565.8099999926|           58.78|                5|\n",
      "|         138|        89533|   65.014798565891|   5820969.959999919|          176.43|                5|\n",
      "|         239|        88474|20.934708275877732|  1852177.3800000064|        15015.12|                5|\n",
      "+------------+-------------+------------------+--------------------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupemos por 'PULocationID' (Zona de recogida)\n",
    "# Calculemos m칰ltiples estad칤sticas sobre esos viajes\n",
    "df_agregado = df_spark.groupBy(\"PULocationID\").agg(\n",
    "    F.count(\"*\").alias(\"conteo_viajes\"),\n",
    "    F.avg(\"total_amount\").alias(\"promedio_total\"),\n",
    "    F.sum(\"total_amount\").alias(\"suma_total\"),\n",
    "    F.max(\"trip_distance\").alias(\"distancia_maxima\"),\n",
    "    F.countDistinct(\"payment_type\").alias(\"tipos_pago_unicos\")\n",
    ")\n",
    "\n",
    "print(\"Agregaci칩n completa por Zona de Recogida:\")\n",
    "df_agregado.orderBy(F.col(\"conteo_viajes\").desc()).show(10) # Ordenamos por los m치s populares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa636a2e-066b-4bd2-89de-c59dd21963f8",
   "metadata": {},
   "source": [
    "**Agregaci칩n Global (Sin `groupBy`)**\n",
    "\n",
    "쯈u칠 pasa si quieres el promedio de `total_amount` de *todo* el DataFrame? No necesitas un `groupBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89ca00bf-6526-4b8b-a680-76d016ac2074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estad칤sticas globales de TODO el dataset:\n",
      "+---------------------+--------------------+---------------+\n",
      "|promedio_total_global|total_tarifas_global|viaje_mas_largo|\n",
      "+---------------------+--------------------+---------------+\n",
      "|   26.801504770952707|5.3882224760004714E7|       312722.3|\n",
      "+---------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usamos .agg() directamente sobre el DataFrame\n",
    "df_stats_globales = df_spark.agg(\n",
    "    F.avg(\"total_amount\").alias(\"promedio_total_global\"),\n",
    "    F.sum(\"fare_amount\").alias(\"total_tarifas_global\"),\n",
    "    F.max(\"trip_distance\").alias(\"viaje_mas_largo\")\n",
    ")\n",
    "\n",
    "print(\"Estad칤sticas globales de TODO el dataset:\")\n",
    "df_stats_globales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a11ba-97b1-4bca-9f8c-0db7f6db0202",
   "metadata": {},
   "source": [
    "### Parte 5: Uniendo DataFrames (El `join`)\n",
    "\n",
    "El `join` es fundamental. Vamos a crear un DataFrame \"diccionario\" para nuestros `payment_type` y a unirlos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875f985-ff81-48ac-9279-552db8f8d9f0",
   "metadata": {},
   "source": [
    "**Creando un DataFrame \"Diccionario\"**\n",
    "\n",
    "Crearemos un peque침o DataFrame de Spark desde cero para mapear los IDs de pago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4876df9-5438-4c26-a7c3-8c49e9524ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:30:40|  2024-01-01 00:58:40|              0|          3.0|         1|                 N|         246|         231|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:26:01|  2024-01-01 00:54:12|              1|         5.44|         1|                 N|         161|         261|           2|       31.0|  1.0|    0.5|       0.0|         0.0|                  1.0|        36.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:28:08|  2024-01-01 00:29:16|              1|         0.04|         1|                 N|         113|         113|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:35:22|  2024-01-01 00:41:41|              2|         0.75|         1|                 N|         107|         137|           1|        7.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:25:00|  2024-01-01 00:34:03|              2|          1.2|         1|                 N|         158|         246|           1|       14.9|  3.5|    0.5|      3.95|         0.0|                  1.0|       23.85|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:35:16|  2024-01-01 01:11:52|              2|          8.2|         1|                 N|         246|         190|           1|       59.0|  3.5|    0.5|     14.15|        6.94|                  1.0|       85.09|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:43:27|  2024-01-01 00:47:11|              2|          0.4|         1|                 N|          68|          90|           1|        5.8|  3.5|    0.5|      1.25|         0.0|                  1.0|       12.05|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:51:53|  2024-01-01 00:55:43|              1|          0.8|         1|                 N|          90|          68|           2|        6.5|  3.5|    0.5|       0.0|         0.0|                  1.0|        11.5|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:50:09|  2024-01-01 01:03:57|              1|          5.0|         1|                 N|         132|         216|           2|       21.2| 2.75|    0.5|       0.0|         0.0|                  1.0|       25.45|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:41:06|  2024-01-01 00:53:42|              1|          1.5|         1|                 N|         164|          79|           1|       12.8|  3.5|    0.5|      4.45|         0.0|                  1.0|       22.25|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:52:09|  2024-01-01 00:52:28|              1|          0.0|         1|                 N|         237|         237|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:56:38|  2024-01-01 01:03:17|              1|          1.5|         1|                 N|         141|         263|           1|        9.3|  1.0|    0.5|       3.0|         0.0|                  1.0|        17.3|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:32:34|  2024-01-01 00:49:33|              1|         2.57|         1|                 N|         161|         263|           1|       17.7|  1.0|    0.5|      10.0|         0.0|                  1.0|        32.7|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ab5d77b-2af1-49ec-bdb6-9363ea41ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro DataFrame diccionario COMPLETO:\n",
      "+----------+------------------+\n",
      "|payment_id|       nombre_pago|\n",
      "+----------+------------------+\n",
      "|         0|       Desconocido|\n",
      "|         1|Tarjeta de Cr칠dito|\n",
      "|         2|          Efectivo|\n",
      "|         3|         Sin Cargo|\n",
      "|         4|           Disputa|\n",
      "|         5|             Vac칤o|\n",
      "|         6|              Nulo|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Definimos los datos y el esquema (incluyendo el '0')\n",
    "# (Una b칰squeda r치pida en Google sobre el dataset nos dice qu칠 son 0, 5, 6)\n",
    "datos_pago = [(0, \"Desconocido\"), # 춰Lo encontramos gracias al groupBy!\n",
    "              (1, \"Tarjeta de Cr칠dito\"),\n",
    "              (2, \"Efectivo\"),\n",
    "              (3, \"Sin Cargo\"),\n",
    "              (4, \"Disputa\"),\n",
    "              (5, \"Vac칤o\"),\n",
    "              (6, \"Nulo\")]\n",
    "esquema_pago = [\"payment_id\", \"nombre_pago\"]\n",
    "\n",
    "# 2. Creamos el DataFrame\n",
    "df_diccionario_pagos = spark.createDataFrame(datos_pago, schema=esquema_pago)\n",
    "\n",
    "print(\"Nuestro DataFrame diccionario COMPLETO:\")\n",
    "df_diccionario_pagos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebbbd4-b756-4f29-bb10-aa4d4a7ea5a2",
   "metadata": {},
   "source": [
    "**`join()` - Uniendo los DataFrames**\n",
    "\n",
    "Ahora, unamos nuestro `df_spark` principal con nuestro `df_diccionario_pagos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18f4d6b8-7077-4472-851a-f883f39f8c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame unido con nombres de pago:\n",
      "+---------------+------------+------------------+\n",
      "|passenger_count|total_amount|       nombre_pago|\n",
      "+---------------+------------+------------------+\n",
      "|              1|       18.75|Tarjeta de Cr칠dito|\n",
      "|              1|        31.3|Tarjeta de Cr칠dito|\n",
      "|              1|        17.0|Tarjeta de Cr칠dito|\n",
      "|              1|        16.1|Tarjeta de Cr칠dito|\n",
      "|              1|        41.5|Tarjeta de Cr칠dito|\n",
      "|              2|       64.95|Tarjeta de Cr칠dito|\n",
      "|              2|        12.9|Tarjeta de Cr칠dito|\n",
      "|              1|        22.7|          Efectivo|\n",
      "|              0|        30.4|          Efectivo|\n",
      "|              1|        36.0|          Efectivo|\n",
      "+---------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La sintaxis del join es:\n",
    "# df_izquierdo.join(df_derecho, <condici칩n_del_join>, <tipo_de_join>)\n",
    "df_unido = df_spark.join(\n",
    "    df_diccionario_pagos,\n",
    "    df_spark.payment_type == df_diccionario_pagos.payment_id, # La condici칩n\n",
    "    \"left_outer\" # Tipo de join (left, inner, right, full_outer)\n",
    ")\n",
    "\n",
    "print(\"DataFrame unido con nombres de pago:\")\n",
    "df_unido.select(\"passenger_count\", \"total_amount\", \"nombre_pago\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cada204-24cd-4a41-868e-2404de51819f",
   "metadata": {},
   "source": [
    "### Parte 6: El Benchmark (춰Ahora s칤!)\n",
    "\n",
    "Ahora repetimos un ejercicio anterior, pero con todo lo que sabemos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb33ede-f219-4d3b-b1ef-bdc9f4c7ce23",
   "metadata": {},
   "source": [
    "**El Benchmark (Desbalanceado vs. Balanceado)**\n",
    "\n",
    "Repetimos el experimento de la \"Clase 2\", pero ahora entendemos cada funci칩n (`filter`, `withColumn`, `groupBy`, `agg`, `orderBy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18b0ab20-3bf2-45a9-85d0-e11911a53cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ejecutando Benchmark 1 (Desbalanceado, 3 n칰cleos activos) ---\n",
      "+------------+----------+--------------------+-------------+\n",
      "|payment_type|tipo_viaje|    propina_promedio|conteo_viajes|\n",
      "+------------+----------+--------------------+-------------+\n",
      "|           1|     Medio|   6.008564521889089|       938015|\n",
      "|           2|     Medio|0.002486501326988...|       163905|\n",
      "|           0|     Medio|   2.234879677758519|        67527|\n",
      "|           1|     Largo|  15.338880528457176|        22859|\n",
      "|           4|     Medio| 0.06863114231014676|        15670|\n",
      "|           2|     Largo|0.032797917470111834|         5186|\n",
      "|           3|     Medio|0.028834973166368513|         4472|\n",
      "|           4|     Largo|  0.3381708945260347|          749|\n",
      "|           0|     Largo|   10.91443762781186|          489|\n",
      "|           3|     Largo| 0.13790960451977402|          177|\n",
      "+------------+----------+--------------------+-------------+\n",
      "\n",
      "CPU times: user 1.8 ms, sys: 957 풮s, total: 2.76 ms\n",
      "Wall time: 514 ms\n",
      "\n",
      "--- Ejecutando Benchmark 2 (Balanceado, 12 n칰cleos activos) ---\n",
      "+------------+----------+--------------------+-------------+\n",
      "|payment_type|tipo_viaje|    propina_promedio|conteo_viajes|\n",
      "+------------+----------+--------------------+-------------+\n",
      "|           1|     Medio|   6.008564521889595|       938015|\n",
      "|           2|     Medio|0.002486501326988...|       163905|\n",
      "|           0|     Medio|   2.234879677758525|        67527|\n",
      "|           1|     Largo|  15.338880528457011|        22859|\n",
      "|           4|     Medio| 0.06863114231014678|        15670|\n",
      "|           2|     Largo|0.032797917470111834|         5186|\n",
      "|           3|     Medio|0.028834973166368513|         4472|\n",
      "|           4|     Largo|  0.3381708945260347|          749|\n",
      "|           0|     Largo|  10.914437627811862|          489|\n",
      "|           3|     Largo| 0.13790960451977402|          177|\n",
      "+------------+----------+--------------------+-------------+\n",
      "\n",
      "CPU times: user 1.83 ms, sys: 405 풮s, total: 2.23 ms\n",
      "Wall time: 212 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp_ntz, tpep_dropoff_datetime: timestamp_ntz, passenger_count: bigint, trip_distance: double, RatecodeID: bigint, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, Airport_fee: double]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Definir la consulta compleja ---\n",
    "def analizar_viajes_completo(dataframe):\n",
    "    return dataframe.filter(\n",
    "        (F.col(\"trip_distance\") > 2) & (F.col(\"trip_distance\") < 100)\n",
    "    ).withColumn(\"tipo_viaje\",\n",
    "        F.when(F.col(\"trip_distance\") > 20, F.lit(\"Largo\"))\n",
    "         .otherwise(F.lit(\"Medio\"))\n",
    "    ).groupBy(\"payment_type\", \"tipo_viaje\").agg(\n",
    "        F.avg(\"tip_amount\").alias(\"propina_promedio\"),\n",
    "        F.count(\"*\").alias(\"conteo_viajes\")\n",
    "    ).orderBy(F.col(\"conteo_viajes\").desc())\n",
    "\n",
    "\n",
    "# --- 2. Preparar los DataFrames (como lo hiciste) ---\n",
    "df_spark.cache()\n",
    "df_spark.count() # Forzar cacheo\n",
    "\n",
    "df_reparticionado = df_spark.repartition(12)\n",
    "df_reparticionado.cache()\n",
    "df_reparticionado.count() # Forzar repartition y cacheo\n",
    "\n",
    "# --- 3. Ejecutar Benchmarks ---\n",
    "print(\"--- Ejecutando Benchmark 1 (Desbalanceado, 3 n칰cleos activos) ---\")\n",
    "%time analizar_viajes_completo(df_spark).show()\n",
    "\n",
    "print(\"\\n--- Ejecutando Benchmark 2 (Balanceado, 12 n칰cleos activos) ---\")\n",
    "%time analizar_viajes_completo(df_reparticionado).show()\n",
    "\n",
    "# Liberamos memoria\n",
    "df_spark.unpersist()\n",
    "df_reparticionado.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ec324-92c6-44f6-84f9-420d05cf8053",
   "metadata": {},
   "source": [
    "춰Aqu칤 es donde todo el trabajo conceptual da sus frutos! Los resultados de tu benchmark son la prueba perfecta de por qu칠 la optimizaci칩n importa.\n",
    "\n",
    "**Tus Resultados:**\n",
    "* **Benchmark 1 (Desbalanceado):** `Wall time: 542 ms`\n",
    "* **Benchmark 2 (Balanceado):** `Wall time: 303 ms`\n",
    "\n",
    "**An치lisis de Rendimiento:**\n",
    "* Al ejecutar `df_spark.repartition(12)`, hiciste que la consulta fuera **춰un 79% m치s r치pida!** (`542 / 303 = 1.79`).\n",
    "* **쯇or qu칠?**\n",
    "    * En el Benchmark 1, las primeras etapas (el `filter` y `withColumn`) se ejecutaron en solo **3 n칰cleos**, creando un \"cuello de botella\". Los otros 9 n칰cleos estaban esperando.\n",
    "    * En el Benchmark 2, esas mismas etapas se ejecutaron en **12 n칰cleos en paralelo**.\n",
    "* **Driver vs. Executors:**\n",
    "    * F칤jate en el `CPU times` (tiempo del Driver): `3.37 ms` vs `6.86 ms`.\n",
    "    * Esto muestra que el Driver (tu notebook) tuvo que \"trabajar\" un poquito m치s en el Benchmark 2. 쯇or qu칠? 춰Porque tuvo que coordinar a 12 trabajadores en lugar de a 3!\n",
    "    * Pero el `Wall time` (tiempo total de reloj) es lo que importa, y se redujo dr치sticamente.\n",
    "\n",
    "**춰Felicidades!** Has demostrado emp칤ricamente c칩mo una optimizaci칩n de distribuci칩n de datos impacta directamente el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7182baa-7a44-431f-9dbc-88c51b11ca99",
   "metadata": {},
   "source": [
    "Hemos cubierto:\n",
    "* **Selecci칩n:** `select`, `selectExpr`\n",
    "* **Filtrado:** `filter`, `where`, `like`, `isin`\n",
    "* **Manipulaci칩n:** `withColumn`, `withColumnRenamed`, `drop`\n",
    "* **Funciones `F`:** `lit`, `when`, `otherwise`, `isNull`, `year`, `month`, `dayofweek`, `hour`, `cast`\n",
    "* **Manejo de Nulos:** `fillna`\n",
    "* **Agregaciones:** `groupBy`, `agg`, `avg`, `sum`, `max`, `count`, `countDistinct`\n",
    "* **Uniones:** `join`, `createDataFrame`\n",
    "* **Acciones:** `show`, `count`, `printSchema` (y las de la clase pasada: `collect`, `write`, `toPandas`)\n",
    "\n",
    "\n",
    "Con esto conformamos un \"Cookbook\" funcional para empezar a trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddaeb9f-e4a7-4177-862a-865fe8c2eebc",
   "metadata": {},
   "source": [
    "### Parte 6: El Siguiente Nivel - Spark SQL y UDFs\n",
    "\n",
    "Has dominado la \"API de DataFrame\" (el estilo Python). Pero Spark tiene otra cara: **Spark SQL**.\n",
    "\n",
    "Adem치s, 쯤u칠 pasa si la funci칩n que quieres no existe en `F`? Creas la tuya: una **UDF** (User Defined Function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7e756-9e86-4687-92f2-017f5591467a",
   "metadata": {},
   "source": [
    "**`createOrReplaceTempView()` - Hablando SQL**\n",
    "\n",
    "Podemos tomar *cualquier* DataFrame (como nuestro `df_unido_completo`) y registrarlo como una \"tabla\" temporal de SQL. Una vez hecho, 춰puedes usar sintaxis SQL pura!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8946e6b0-aacd-4a3a-9a83-0f1900bdac74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "춰El mismo resultado, pero generado con 100% SQL!\n",
      "+------------+------------------+----------+--------------------+-------------+\n",
      "|payment_type|       nombre_pago|tipo_viaje|    propina_promedio|conteo_viajes|\n",
      "+------------+------------------+----------+--------------------+-------------+\n",
      "|           1|Tarjeta de Cr칠dito|     Medio|    6.00856452188944|       938015|\n",
      "|           2|          Efectivo|     Medio|0.002486501326988...|       163905|\n",
      "|           0|       Desconocido|     Medio|   2.234879677758519|        67527|\n",
      "|           1|Tarjeta de Cr칠dito|     Largo|  15.338880528457112|        22859|\n",
      "|           4|           Disputa|     Medio| 0.06863114231014678|        15670|\n",
      "|           2|          Efectivo|     Largo|0.032797917470111834|         5186|\n",
      "|           3|         Sin Cargo|     Medio| 0.02883497316636852|         4472|\n",
      "|           4|           Disputa|     Largo|  0.3381708945260347|          749|\n",
      "|           0|       Desconocido|     Largo|   10.91443762781186|          489|\n",
      "|           3|         Sin Cargo|     Largo| 0.13790960451977402|          177|\n",
      "+------------+------------------+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Registramos nuestro DataFrame como una tabla SQL temporal\n",
    "df_unido.createOrReplaceTempView(\"taxis\")\n",
    "\n",
    "# 2. 춰Escribimos SQL!\n",
    "# Esto hace LO MISMO que nuestro \"analizar_viajes_completo\"\n",
    "df_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        payment_type, \n",
    "        nombre_pago,\n",
    "        CASE \n",
    "            WHEN trip_distance > 20 THEN 'Largo'\n",
    "            ELSE 'Medio'\n",
    "        END AS tipo_viaje,\n",
    "        AVG(tip_amount) AS propina_promedio,\n",
    "        COUNT(*) AS conteo_viajes\n",
    "    FROM taxis\n",
    "    WHERE trip_distance > 2 AND trip_distance < 100\n",
    "    GROUP BY payment_type, nombre_pago, tipo_viaje\n",
    "    ORDER BY conteo_viajes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"춰El mismo resultado, pero generado con 100% SQL!\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff908147-b6d6-4cca-bd4b-f64af729d63f",
   "metadata": {},
   "source": [
    "**`F.udf()` - Creando tus propias funciones**\n",
    "\n",
    "쯈u칠 pasa si queremos una funci칩n compleja que no existe en Spark? Por ejemplo, una que clasifique la propina en \"Baja\", \"Media\", \"Alta\".\n",
    "\n",
    "**Advertencia:** Las UDF son *lentas*. Spark tiene que enviar los datos de su motor optimizado (JVM) a Python, ejecutar tu funci칩n, y devolver el resultado. **Siempre prefiere funciones `F` nativas si puedes.** Pero a veces, son inevitables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee1981ae-ab0b-4f51-940d-1017005c16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame con nuestra UDF personalizada:\n",
      "+----------+------------+-----------------+\n",
      "|tip_amount|total_amount|categoria_propina|\n",
      "+----------+------------+-----------------+\n",
      "|       0.0|        22.7|          Ninguna|\n",
      "|      3.75|       18.75|            Media|\n",
      "|       3.0|        31.3|             Baja|\n",
      "|       2.0|        17.0|             Baja|\n",
      "|       3.2|        16.1|            Media|\n",
      "|       6.9|        41.5|            Media|\n",
      "|      10.0|       64.95|            Media|\n",
      "|       0.0|        30.4|          Ninguna|\n",
      "|       0.0|        36.0|          Ninguna|\n",
      "|       0.0|         8.0|          Ninguna|\n",
      "+----------+------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------------+-------+\n",
      "|categoria_propina|  count|\n",
      "+-----------------+-------+\n",
      "|            Media|1638576|\n",
      "|          Ninguna| 710378|\n",
      "|             Baja| 599218|\n",
      "|             Alta|  16036|\n",
      "|              N/A|    416|\n",
      "+-----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 1. Definimos una funci칩n de Python normal\n",
    "def clasificar_propina(propina, total):\n",
    "    if total is None or total == 0:\n",
    "        return \"N/A\"\n",
    "    if propina is None:\n",
    "        propina = 0\n",
    "    \n",
    "    pct = (propina / total) * 100\n",
    "    \n",
    "    if pct > 25:\n",
    "        return \"Alta\"\n",
    "    elif pct > 15:\n",
    "        return \"Media\"\n",
    "    elif pct > 0:\n",
    "        return \"Baja\"\n",
    "    else:\n",
    "        return \"Ninguna\"\n",
    "\n",
    "# 2. \"Envolvemos\" nuestra funci칩n de Python en una UDF de Spark\n",
    "# Le decimos a Spark el tipo de dato que nuestra funci칩n va a devolver\n",
    "clasificar_propina_udf = F.udf(clasificar_propina, StringType())\n",
    "\n",
    "# 3. Usamos la UDF como si fuera una funci칩n F\n",
    "df_con_udf = df_spark.withColumn(\"categoria_propina\", \n",
    "    clasificar_propina_udf(F.col(\"tip_amount\"), F.col(\"total_amount\"))\n",
    ")\n",
    "\n",
    "print(\"DataFrame con nuestra UDF personalizada:\")\n",
    "df_con_udf.select(\"tip_amount\", \"total_amount\", \"categoria_propina\").show(10)\n",
    "\n",
    "# 춰Ahora podemos contar las categor칤as!\n",
    "df_con_udf.groupBy(\"categoria_propina\").count().orderBy(F.col(\"count\").desc()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
