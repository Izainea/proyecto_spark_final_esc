{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "465a3b20",
   "metadata": {},
   "source": [
    "### Verificaci칩n del Entorno MLOps\n",
    "\n",
    "Este documento te guiar치 para validar que todos los servicios de tu infraestructura MLOps (Jupyter, Spark, MLflow, Kafka y Airflow) est치n activos y se comunican correctamente.\n",
    "\n",
    "#### 1\\. Verificar Contenedores Activos\n",
    "\n",
    "Primero, aseg칰rate de que todos los contenedores definidos en tu `docker-compose.yml` est칠n corriendo.\n",
    "\n",
    "  * **Comando:**\n",
    "\n",
    "    ```bash\n",
    "    docker ps\n",
    "    ```\n",
    "\n",
    "  * **Resultado Esperado:** Deber칤as ver una lista con contenedores para los siguientes servicios (los nombres pueden variar ligeramente seg칰n tu configuraci칩n):\n",
    "\n",
    "      * `python_ml_stack` (Jupyter/Spark/MLflow)\n",
    "      * `postgres_db`\n",
    "      * `kafka`\n",
    "      * `zookeeper`\n",
    "      * `airflow_webserver`\n",
    "      * `airflow_scheduler`\n",
    "\n",
    "Si alg칰n contenedor no est치 en la lista o su estado es `Exited`, revisa los logs con `docker logs <nombre_contenedor>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435292c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### 2\\. Acceder a las Interfaces Web (UIs)\n",
    "\n",
    "Verifica que puedes acceder a las interfaces gr치ficas de los servicios desde tu navegador.\n",
    "\n",
    "  * **JupyterLab:**\n",
    "\n",
    "      * URL: `http://localhost:8888`\n",
    "      * Token: El que definiste en tu `Dockerfile` (`hola-mundo` seg칰n tu configuraci칩n actual) o el que aparece en los logs de inicio.\n",
    "      * **Prueba:** Intenta crear un nuevo notebook.\n",
    "\n",
    "  * **Spark Master UI:**\n",
    "\n",
    "      * URL: `http://localhost:8080`\n",
    "      * **Prueba:** Verifica que aparezca el Worker conectado y que haya recursos disponibles (Cores/Memory).\n",
    "\n",
    "  * **MLflow UI:**\n",
    "\n",
    "      * URL: `http://localhost:5000`\n",
    "      * **Prueba:** Deber칤as ver la interfaz de MLflow. Intenta crear un experimento de prueba desde un notebook (ver secci칩n 4).\n",
    "\n",
    "  * **Airflow Webserver:**\n",
    "\n",
    "      * URL: `http://localhost:8081`\n",
    "      * Credenciales: `admin` / `admin` (seg칰n las variables de entorno configuradas).\n",
    "      * **Prueba:** Inicia sesi칩n y verifica que puedes ver la lista de DAGs de ejemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4eacbb",
   "metadata": {},
   "source": [
    "#### 3\\. Probar la Conexi칩n con Kafka\n",
    "\n",
    "Para validar que Kafka y Zookeeper funcionan y son accesibles desde tu entorno de Python (Jupyter), ejecuta el siguiente script en un notebook de Jupyter.\n",
    "\n",
    "  * **Prueba (Kafka):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e54bfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Productor Kafka conectado exitosamente.\n",
      "Mensaje enviado a 'test_topic': {'mensaje': 'Hola desde Jupyter!', 'id': 1}\n",
      "Consumidor Kafka conectado exitosamente.\n",
      "Mensaje recibido: {'mensaje': 'Hola desde Jupyter!', 'id': 1}\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Configuraci칩n\n",
    "KAFKA_TOPIC = 'test_topic'\n",
    "KAFKA_BOOTSTRAP_SERVERS = 'kafka:29092' # Usamos el nombre del servicio definido en docker-compose\n",
    "\n",
    "# 1. Crear Productor\n",
    "try:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    print(\"Productor Kafka conectado exitosamente.\")\n",
    "\n",
    "    # Enviar mensaje\n",
    "    message = {'mensaje': 'Hola desde Jupyter!', 'id': 1}\n",
    "    producer.send(KAFKA_TOPIC, message)\n",
    "    producer.flush()\n",
    "    print(f\"Mensaje enviado a '{KAFKA_TOPIC}': {message}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error conectando Productor: {e}\")\n",
    "\n",
    "# 2. Crear Consumidor\n",
    "try:\n",
    "    consumer = KafkaConsumer(\n",
    "        KAFKA_TOPIC,\n",
    "        bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "        auto_offset_reset='earliest', # Leer desde el principio\n",
    "        enable_auto_commit=True,\n",
    "        group_id='my-group',\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "        consumer_timeout_ms=5000 # Esperar 5 segundos y salir si no hay mensajes\n",
    "    )\n",
    "    print(\"Consumidor Kafka conectado exitosamente.\")\n",
    "\n",
    "    # Leer mensaje\n",
    "    for msg in consumer:\n",
    "        print(f\"Mensaje recibido: {msg.value}\")\n",
    "        break # Solo queremos probar que llega uno\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error conectando Consumidor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b437b",
   "metadata": {},
   "source": [
    "#### 4\\. Probar Spark + MLflow\n",
    "\n",
    "Verifica que Spark pueda procesar datos y registrar experimentos en MLflow. Puedes reusar o adaptar el c칩digo de tu `Cuaderno_1.ipynb`, asegur치ndote de que la URI de MLflow sea correcta.\n",
    "\n",
    "  * **Notebook de Prueba (Spark + MLflow):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be3b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/01 04:24:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/01 04:24:01 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run registrado en MLflow\n",
      "游끢 View run painted-moose-307 at: http://localhost:5000/#/experiments/2/runs/044b780dff0e4c5c8040c5102e8b4e6e\n",
      "游빍 View experiment at: http://localhost:5000/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "\n",
    "# 1. Inicializar Spark en modo LOCAL\n",
    "# Usamos \"local[*]\" para usar todos los n칰cleos del contenedor sin necesitar red\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test_Spark_MLflow\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# 2. Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Test_Experiment\")\n",
    "\n",
    "# 3. Registrar un run simple\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"metric1\", 0.85)\n",
    "    print(\"Run registrado en MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f3bff",
   "metadata": {},
   "source": [
    "#### 5\\. Probar Airflow (Opcional)\n",
    "\n",
    "Para verificar que Airflow est치 escaneando correctamente tu carpeta `dags`, crea un archivo de prueba simple en tu carpeta local `dags/`.\n",
    "  * **Archivo `dags/test_dag.py`:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccc043-b1ac-4ab3-97c9-06a741e67b02",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    'test_dag_simple',\n",
    "    start_date=datetime(2023, 1, 1),\n",
    "    schedule_interval=None,\n",
    "    catchup=False\n",
    ") as dag:\n",
    "\n",
    "    t1 = BashOperator(\n",
    "        task_id='print_date',\n",
    "        bash_command='date',\n",
    "    )\n",
    "\n",
    "    t2 = BashOperator(\n",
    "        task_id='echo_hello',\n",
    "        bash_command='echo \"Hola Airflow!\"',\n",
    "    )\n",
    "\n",
    "    t1 >> t2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4c4c11",
   "metadata": {},
   "source": [
    "  * **Validaci칩n:**\n",
    "\n",
    "    1.  Guarda el archivo.\n",
    "    2.  Espera unos segundos y refresca la UI de Airflow (`http://localhost:8081`).\n",
    "    3.  Deber칤as ver `test_dag_simple` en la lista.\n",
    "    4.  Act칤valo (toggle \"On\") y ejec칰talo manualmente (bot칩n \"Play\").\n",
    "    5.  Verifica que las tareas se pongan en verde (Success).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
