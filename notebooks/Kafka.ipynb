{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka y Spark Streaming: Fundamentos y Práctica\n",
    "\n",
    "Este cuaderno proporciona una explicación detallada de Apache Kafka, su arquitectura, su integración con Apache Spark Streaming y un ejemplo práctico simulado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Apache Kafka?\n",
    "\n",
    "Apache Kafka es una plataforma distribuida de transmisión de eventos (streaming) diseñada para manejar flujos de datos en tiempo real. Es escalable, tolerante a fallos y de alto rendimiento.\n",
    "\n",
    "### Conceptos Clave:\n",
    "\n",
    "* **Evento (Mensaje):** La unidad básica de datos en Kafka. Es un registro de algo que sucedió (ej. \"usuario X hizo clic en botón Y\").\n",
    "* **Productor (Producer):** Aplicación que publica (escribe) eventos en Kafka.\n",
    "* **Consumidor (Consumer):** Aplicación que se suscribe a (lee) y procesa eventos de Kafka.\n",
    "* **Tópico (Topic):** Categoría donde se organizan y almacenan los eventos. Similar a una carpeta en un sistema de archivos o una tabla en una base de datos.\n",
    "* **Partición (Partition):** Los tópicos se dividen en particiones para permitir el paralelismo. Cada partición es una secuencia ordenada e inmutable de registros.\n",
    "* **Broker:** Servidor que almacena los datos. Un clúster de Kafka está formado por múltiples brokers.\n",
    "* **Offset:** Identificador único secuencial asignado a cada mensaje dentro de una partición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Arquitectura de Kafka\n",
    "\n",
    "La arquitectura de Kafka se basa en un registro de confirmación (commit log) distribuido. Los productores añaden mensajes al final del log, y los consumidores leen desde una posición específica (offset).\n",
    "\n",
    "### Flujo de Datos:\n",
    "\n",
    "1.  **Productor** envía un mensaje a un **Tópico**.\n",
    "2.  El mensaje se almacena en una **Partición** de un **Broker**.\n",
    "3.  Un **Consumidor** (o grupo de consumidores) lee el mensaje del Tópico.\n",
    "4.  El Consumidor procesa el mensaje y actualiza su **Offset** para saber qué ha leído."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rol de Kafka en esta Aplicación MLOps\n",
    "\n",
    "En este proyecto, Kafka actúa como el sistema nervioso central para la ingesta de datos en tiempo real.\n",
    "\n",
    "### Escenario:\n",
    "\n",
    "1.  **Simulación de Datos (Productor):** Generaremos datos sintéticos de viajes de taxi (similar al dataset de Nueva York) que simulan eventos en tiempo real.\n",
    "2.  **Ingesta (Kafka):** Estos datos se envían a un tópico de Kafka llamado `taxi_trips`.\n",
    "3.  **Procesamiento (Spark Streaming):** Spark lee estos datos desde Kafka, realiza transformaciones y extracciones de características (feature engineering) al vuelo.\n",
    "4.  **Almacenamiento/ML (MLflow/Postgres):** Los datos procesados pueden usarse para re-entrenar modelos o para inferencia en tiempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuración del Entorno\n",
    "\n",
    "Primero, asegurémonos de tener las librerías necesarias instaladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creación del Productor de Kafka (Simulador)\n",
    "\n",
    "Vamos a crear un productor que envíe datos JSON simulados de viajes de taxi a un tópico de Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuración de Kafka\n",
    "# Usamos el puerto interno definido en KAFKA_ADVERTISED_LISTENERS (PLAINTEXT)\n",
    "KAFKA_BROKER = 'kafka:29092' \n",
    "TOPIC_NAME = 'taxi_trips'\n",
    "\n",
    "# Inicializar el Productor\n",
    "try:\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=KAFKA_BROKER,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "        request_timeout_ms=5000  # 5 segundos de tiempo de espera\n",
    "    )\n",
    "    print(f\"Productor Kafka inicializado correctamente conectando a {KAFKA_BROKER}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al conectar con Kafka: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para Generar Datos de Taxi Simulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trip_data():\n",
    "    \"\"\"Genera un diccionario con datos simulados de un viaje de taxi.\"\"\"\n",
    "    return {\n",
    "        'vendor_id': random.choice([1, 2]),\n",
    "        'pickup_datetime': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        'passenger_count': random.randint(1, 6),\n",
    "        'trip_distance': round(random.uniform(0.5, 20.0), 2),\n",
    "        'pickup_longitude': round(random.uniform(-74.0, -73.9), 6),\n",
    "        'pickup_latitude': round(random.uniform(40.7, 40.8), 6),\n",
    "        'dropoff_longitude': round(random.uniform(-74.0, -73.9), 6),\n",
    "        'dropoff_latitude': round(random.uniform(40.7, 40.8), 6),\n",
    "        'payment_type': random.choice([1, 2]), # 1=Credit Card, 2=Cash\n",
    "        'fare_amount': round(random.uniform(5.0, 50.0), 2),\n",
    "        'tip_amount': round(random.uniform(0.0, 10.0), 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviar Datos al Tópico (Ejecutar esto en segundo plano o detener manualmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviar 10 mensajes como prueba\n",
    "print(f\"Enviando mensajes al tópico '{TOPIC_NAME}'...\")\n",
    "for i in range(10):\n",
    "    data = generate_trip_data()\n",
    "    producer.send(TOPIC_NAME, data)\n",
    "    print(f\"Enviado: {data}\")\n",
    "    time.sleep(1) # Simular llegada en tiempo real\n",
    "\n",
    "producer.flush()\n",
    "print(\"Envío completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuración de Spark Streaming\n",
    "\n",
    "Ahora configuraremos Spark para leer estos datos desde Kafka. Spark Structured Streaming trata los datos de Kafka como un DataFrame infinito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Iniciar Spark Session con soporte para Kafka\n",
    "# Nota: En un entorno real, se deben incluir los paquetes de spark-sql-kafka\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"KafkaSparkStreaming\") \\\n",
    "#     .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Para este ejemplo simulado en el entorno actual, usaremos una sesión básica\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreamingBasic\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Definición del Esquema de Datos\n",
    "\n",
    "Kafka envía datos como bytes. Necesitamos decirle a Spark cómo interpretar el JSON que estamos enviando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"vendor_id\", IntegerType()),\n",
    "    StructField(\"pickup_datetime\", StringType()),\n",
    "    StructField(\"passenger_count\", IntegerType()),\n",
    "    StructField(\"trip_distance\", DoubleType()),\n",
    "    StructField(\"pickup_longitude\", DoubleType()),\n",
    "    StructField(\"pickup_latitude\", DoubleType()),\n",
    "    StructField(\"dropoff_longitude\", DoubleType()),\n",
    "    StructField(\"dropoff_latitude\", DoubleType()),\n",
    "    StructField(\"payment_type\", IntegerType()),\n",
    "    StructField(\"fare_amount\", DoubleType()),\n",
    "    StructField(\"tip_amount\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lectura del Stream (Simulación)\n",
    "\n",
    "El siguiente código muestra cómo se configuraría la lectura del stream desde Kafka. \n",
    "\n",
    "**Nota:** Como la ejecución de `readStream` es bloqueante y requiere el paquete de Kafka (que se descarga al inicio), aquí mostraremos la lógica. En tu entorno Docker con las dependencias correctas, esto funcionará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULACIÓN: Como no tenemos los JARs de Kafka-Spark cargados en esta sesión básica,\n",
    "# usaremos el formato 'rate' para simular un stream y poder ejecutar el resto del código.\n",
    "# En producción, descomenta las líneas de Kafka.\n",
    "\n",
    "# df_raw = spark.readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "#     .option(\"subscribe\", TOPIC_NAME) \\\n",
    "#     .option(\"startingOffsets\", \"earliest\") \\\n",
    "#     .load()\n",
    "\n",
    "# Simulación con 'rate' (genera filas con timestamp y value)\n",
    "df_raw = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# Para la simulación, crearemos columnas dummy para que coincidan con el esquema esperado\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_parsed = df_raw.withColumn(\"fare_amount\", (col(\"value\") % 50).cast(\"double\")) \\\n",
    "                  .withColumn(\"tip_amount\", (col(\"value\") % 10).cast(\"double\")) \\\n",
    "                  .withColumn(\"vendor_id\", lit(1)) \\\n",
    "                  .select(\"fare_amount\", \"tip_amount\", \"vendor_id\")\n",
    "\n",
    "print(\"Stream de lectura inicializado (Simulación).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transformaciones en Tiempo Real\n",
    "\n",
    "Una vez que tenemos el DataFrame de streaming, podemos aplicar transformaciones como si fuera un DataFrame estático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de transformaciones sobre el stream\n",
    "def process_stream(df_stream):\n",
    "    # 1. Calcular el costo total (tarifa + propina)\n",
    "    df_out = df_stream.withColumn(\"total_cost\", col(\"fare_amount\") + col(\"tip_amount\"))\n",
    "    \n",
    "    # 2. Filtrar viajes con costo negativo o cero (datos sucios)\n",
    "    df_out = df_out.filter(col(\"total_cost\") > 0)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# APLICAMOS LA TRANSFORMACIÓN\n",
    "# Esto crea el DataFrame 'df_processed' que faltaba antes\n",
    "df_processed = process_stream(df_parsed)\n",
    "\n",
    "print(\"Transformaciones aplicadas. DataFrame 'df_processed' listo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Salida del Stream (Sink)\n",
    "\n",
    "Finalmente, debemos decidir dónde enviar los resultados. Para depuración, usamos `console` o `memory`. En producción, podría ser otra base de datos, un archivo Parquet o incluso otro tópico de Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el stream a la consola (para ver los resultados)\n",
    "# Usamos 'append' porque no hay agregaciones\n",
    "try:\n",
    "    query = df_processed.writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "\n",
    "    # Esperar unos segundos para ver la salida y luego detener\n",
    "    time.sleep(10)\n",
    "    query.stop()\n",
    "    print(\"Streaming finalizado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en streaming: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Consumidor de Prueba con Python (kafka-python)\n",
    "\n",
    "Para verificar que los datos se están escribiendo correctamente en Kafka sin depender de Spark Streaming por ahora, usaremos un consumidor simple de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Inicializar Consumidor\n",
    "try:\n",
    "    consumer = KafkaConsumer(\n",
    "        TOPIC_NAME,\n",
    "        bootstrap_servers=KAFKA_BROKER,\n",
    "        auto_offset_reset='earliest', # Leer desde el principio si no hay offset guardado\n",
    "        enable_auto_commit=True,\n",
    "        value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
    "        consumer_timeout_ms=5000 # Dejar de escuchar después de 5 segundos de inactividad\n",
    "    )\n",
    "    print(\"Consumidor Kafka inicializado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al conectar Consumidor: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer Mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Leyendo mensajes del tópico '{TOPIC_NAME}'...\")\n",
    "msg_count = 0\n",
    "for message in consumer:\n",
    "    print(f\"Recibido: {message.value}\")\n",
    "    msg_count += 1\n",
    "    if msg_count >= 5: # Leer solo 5 mensajes para el ejemplo\n",
    "        break\n",
    "print(\"Lectura finalizada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Concepto Avanzado: Window Operations en Spark Streaming\n",
    "\n",
    "Una de las capacidades más potentes de Spark Streaming es realizar agregaciones sobre ventanas de tiempo (ej. \"promedio de propina en los últimos 10 minutos\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Ejemplo conceptual de agregación por ventana\n",
    "def aggregate_by_window(df_stream):\n",
    "    # Asegurarse de que hay una columna de timestamp\n",
    "    # df_stream = df_stream.withColumn(\"timestamp\", ...)\n",
    "    \n",
    "    # Agrupar por ventanas de 10 minutos, deslizándose cada 5 minutos\n",
    "    # windowed_counts = df_stream.groupBy(\n",
    "    #     window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\"),\n",
    "    #     col(\"vendor_id\")\n",
    "    # ).count()\n",
    "    \n",
    "    # return windowed_counts\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Integración con MLflow (Conceptual)\n",
    "\n",
    "¿Cómo encaja MLflow aquí? \n",
    "\n",
    "1.  **Entrenamiento:** Spark lee un lote histórico de Kafka (o data lake), entrena un modelo y lo guarda en MLflow.\n",
    "2.  **Inferencia:** Spark Streaming carga el modelo desde MLflow (`model = mlflow.spark.load_model(...)`) y lo aplica a cada micro-lote de datos que llega de Kafka para hacer predicciones en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.spark\n",
    "\n",
    "# Cargar un modelo previamente entrenado\n",
    "# model_uri = \"models:/TaxiFarePrediction/Production\"\n",
    "# model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "# Aplicar predicción al stream\n",
    "# prediction = model.transform(df_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Monitoreo y Checkpointing\n",
    "\n",
    "En aplicaciones de streaming, es crucial guardar el estado (checkpointing) para recuperarse de fallos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Checkpoint\n",
    "CHECKPOINT_DIR = \"/tmp/spark_checkpoints/taxi_app\"\n",
    "\n",
    "# query = df_processed.writeStream \\\n",
    "#     .option(\"checkpointLocation\", CHECKPOINT_DIR) \\\n",
    "#     ...\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Resumen del Flujo de Datos\n",
    "\n",
    "1.  **Fuente:** Script Python (Simulador) -> Kafka Topic `taxi_trips`.\n",
    "2.  **Canal:** Kafka Broker gestiona la cola de mensajes.\n",
    "3.  **Procesador:** Spark Streaming lee, limpia y agrega datos.\n",
    "4.  **Inteligencia:** Modelo de MLflow aplica predicciones.\n",
    "5.  **Destino:** Resultados se muestran en consola o se guardan en base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Limpieza de Recursos\n",
    "\n",
    "Es buena práctica cerrar las conexiones al finalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    producer.close()\n",
    "    consumer.close()\n",
    "    print(\"Conexiones a Kafka cerradas.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark.stop()\n",
    "print(\"Spark Session detenida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Anexo: Comandos Útiles de Kafka (CLI)\n",
    "\n",
    "Si necesitas depurar Kafka desde la terminal del contenedor:\n",
    "\n",
    "* **Listar tópicos:** `kafka-topics --list --bootstrap-server localhost:9092`\n",
    "* **Crear tópico:** `kafka-topics --create --topic test --bootstrap-server localhost:9092`\n",
    "* **Consumir consola:** `kafka-console-consumer --topic test --from-beginning --bootstrap-server localhost:9092`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Próximos Pasos\n",
    "\n",
    "1.  Implementar el DAG de Airflow para orquestar el re-entrenamiento del modelo.\n",
    "2.  Conectar Spark Streaming a una base de datos real (Postgres) para persistir resultados.\n",
    "3.  Visualizar los datos en tiempo real con una herramienta de dashboarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Fin del Cuaderno ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}