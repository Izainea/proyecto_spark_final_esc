# **Proyecto End-to-End: MLOps & Big Data Streaming Pipeline**

Este proyecto implementa una plataforma completa de **Ingenier√≠a de Datos y MLOps** utilizando tecnolog√≠as de vanguardia contenerizadas con Docker. Simula un entorno productivo real para la ingesta, procesamiento, entrenamiento y orquestaci√≥n de modelos de Machine Learning a escala.

## **üèóÔ∏è Arquitectura del Proyecto**

El sistema est√° dise√±ado de forma modular, donde cada componente cumple un rol espec√≠fico en el ciclo de vida del dato:

1. **Ingesta en Tiempo Real (Apache Kafka):** Act√∫a como el bus de mensajer√≠a central. Recibe datos simulados (ej. viajes de taxi) y los disponibiliza para su consumo inmediato.  
2. **Procesamiento Distribuido (Apache Spark):**  
   * **Batch:** Procesa grandes vol√∫menes de datos hist√≥ricos (archivos Parquet).  
   * **Streaming:** Consume datos en tiempo real desde Kafka para an√°lisis al vuelo.  
3. **Gesti√≥n del Ciclo de Vida de ML (MLflow):** Rastrea experimentos, registra m√©tricas/par√°metros y versiona los modelos entrenados (Model Registry) utilizando PostgreSQL como backend.  
4. **Orquestaci√≥n (Apache Airflow):** Programa y monitorea los flujos de trabajo (DAGs), asegurando que las tareas de ETL y re-entrenamiento se ejecuten en el orden y tiempo correctos.  
5. **Almacenamiento (PostgreSQL):** Base de datos relacional utilizada como *backend store* para los metadatos de Airflow y MLflow.

## **üöÄ Servicios y Puertos**

Una vez desplegado el stack, podr√°s acceder a las siguientes interfaces web:

| Servicio | URL | Descripci√≥n |
| :---- | :---- | :---- |
| **JupyterLab** | http://localhost:8888 | Entorno de desarrollo (Notebooks). Token por defecto: hola-mundo. |
| **Spark Master** | http://localhost:8080 | Monitor del cl√∫ster de Spark y sus trabajadores. |
| **MLflow UI** | http://localhost:5000 | Interfaz para ver experimentos y modelos registrados. |
| **Airflow UI** | http://localhost:8081 | Gesti√≥n de DAGs y tareas. (User/Pass: admin/admin). |
| **Kafka** | localhost:9092 | Broker de mensajer√≠a (Acceso interno/externo configurado). |

## **üìã Requisitos Previos**

* **Docker Desktop** instalado y corriendo.  
  * *Recomendaci√≥n:* Asignar al menos **6GB o 8GB de RAM** a Docker, ya que este stack es intensivo en memoria.  
* **Git** (opcional, para clonar el repo).

## **üõ†Ô∏è Instalaci√≥n y Despliegue**

1. Clonar/Descargar el proyecto:  
   Aseg√∫rate de tener los archivos docker-compose.yml, Dockerfile y las carpetas notebooks/ y dags/ en la ra√≠z.  
2. **Crear carpetas de vol√∫menes (si no existen):**  
   mkdir \-p dags notebooks

3. Construir y Levantar los contenedores:  
   Ejecuta el siguiente comando en la terminal dentro de la carpeta del proyecto:  
   docker-compose up \--build \-d

4. **Verificar estado:**  
   docker ps

   Todos los contenedores (python\_ml\_stack, kafka, airflow\_webserver, etc.) deben estar en estado "Up".**Nota:** Airflow puede tardar unos minutos en iniciar completamente mientras configura su base de datos.

## **üìì Gu√≠a de Uso R√°pida**

### **1\. Exploraci√≥n de Datos (Batch)**

Abre notebooks/Cuaderno Taller.ipynb. Este cuaderno descarga datos hist√≥ricos de taxis de NY, realiza limpieza (ETL) y entrena un modelo de Clustering (K-Means) usando Spark MLlib.

### **2\. MLOps y Registro de Modelos**

Abre notebooks/Entrenamiento.ipynb. Aqu√≠ se entrena un modelo de clasificaci√≥n (Regresi√≥n Log√≠stica) y se utiliza mlflow para registrar los par√°metros, m√©tricas y el modelo serializado.

* Verifica el registro en la UI de MLflow (http://localhost:5000).

### **3\. Streaming en Tiempo Real (Kafka \+ Spark)**

Abre notebooks/kafka\_spark\_streaming.ipynb.

* **Paso A:** Ejecuta las celdas del **Productor** para empezar a enviar datos sint√©ticos a Kafka.  
* **Paso B:** Ejecuta las celdas del **Consumidor (Spark Streaming)** para leer, procesar y visualizar esos datos en tiempo real.

### **4\. Orquestaci√≥n (Airflow)**

Coloca tus scripts de Python (.py) dentro de la carpeta dags/.

* Accede a http://localhost:8081.  
* Activa el DAG y monitorea su ejecuci√≥n gr√°fica.

## **üîß Soluci√≥n de Problemas Comunes**

* **Error de Memoria (OOM) / Contenedores se detienen:**  
  * Aumenta la RAM asignada a Docker Desktop (Settings \-\> Resources).  
* **Airflow pide credenciales o da error de DB:**  
  * Si cambiaste las contrase√±as en el docker-compose.yml, es necesario borrar el volumen de la base de datos antigua para que se regenere:  
    docker-compose down \-\> docker volume rm \<nombre\_volumen\_postgres\> \-\> docker-compose up \-d.  
* **Spark no conecta a Kafka:**  
  * Aseg√∫rate de usar KAFKA\_BROKER \= 'kafka:29092' dentro de los notebooks (comunicaci√≥n interna de Docker).

Tecnolog√≠as: Python 3.11, Spark 3.5, MLflow, Kafka, Airflow, Docker.
